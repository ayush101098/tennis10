{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a1f7158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries loaded\n",
      "Backtesting started at: 2025-12-28 04:54:20\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Import custom modules\n",
    "from features import TennisFeatureExtractor\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "print(\"‚úÖ Libraries loaded\")\n",
    "print(f\"Backtesting started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706b4c28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49653970",
   "metadata": {},
   "source": [
    "## 1. Load Test Data (2023-2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bb81bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test matches: 6,001\n",
      "Date range: 2023-01-02 to 2024-12-18\n"
     ]
    }
   ],
   "source": [
    "# Connect to database\n",
    "conn = sqlite3.connect('tennis_data.db')\n",
    "\n",
    "# Load test matches (2023-2024)\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    m.match_id,\n",
    "    m.tournament_date,\n",
    "    m.surface,\n",
    "    m.winner_id,\n",
    "    m.loser_id,\n",
    "    m.best_of\n",
    "FROM matches m\n",
    "WHERE m.tournament_date >= '2023-01-01'\n",
    "    AND m.tournament_date < '2025-01-01'\n",
    "    AND m.surface IS NOT NULL\n",
    "ORDER BY m.tournament_date\n",
    "\"\"\"\n",
    "\n",
    "test_matches = pd.read_sql_query(query, conn)\n",
    "\n",
    "print(f\"Test matches: {len(test_matches):,}\")\n",
    "print(f\"Date range: {test_matches['tournament_date'].min()} to {test_matches['tournament_date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4645ec45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trained models...\n",
      "‚úÖ Logistic Regression: 9 features\n",
      "   Features: ['WIN_RATE_DIFF', 'DIRECT_H2H', 'SECOND_SERVE_WIN_PCT_DIFF', 'SURFACE_EXP_DIFF', 'COMPLETE_DIFF', 'BP_SAVE_DIFF', 'FIRST_SERVE_WIN_PCT_DIFF', 'FIRST_SERVE_PCT_DIFF', 'SURFACE_WIN_RATE_DIFF']\n",
      "‚úÖ Neural Network Ensemble: 20 models, 17 features\n"
     ]
    }
   ],
   "source": [
    "# Load trained models\n",
    "print(\"Loading trained models...\")\n",
    "\n",
    "# Load logistic regression model\n",
    "with open('ml_models/logistic_regression_trained.pkl', 'rb') as f:\n",
    "    lr_data = pickle.load(f)\n",
    "    lr_model = lr_data['model']\n",
    "    lr_features = lr_data['selected_features']\n",
    "\n",
    "print(f\"‚úÖ Logistic Regression: {len(lr_features)} features\")\n",
    "print(f\"   Features: {lr_features}\")\n",
    "\n",
    "# Load neural network ensemble\n",
    "with open('ml_models/neural_network_ensemble.pkl', 'rb') as f:\n",
    "    nn_data = pickle.load(f)\n",
    "    nn_scaler = nn_data['scaler']\n",
    "    nn_features = nn_data['features']\n",
    "    nn_hidden_dim = nn_data['hidden_dim']\n",
    "    nn_model_states = nn_data['models']\n",
    "\n",
    "# Recreate neural network architecture\n",
    "class SymmetricNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=100):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim, bias=False)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x.squeeze()\n",
    "\n",
    "# Load ensemble models\n",
    "nn_models = []\n",
    "for state_dict in nn_model_states:\n",
    "    model = SymmetricNeuralNetwork(len(nn_features), nn_hidden_dim)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    nn_models.append(model)\n",
    "\n",
    "print(f\"‚úÖ Neural Network Ensemble: {len(nn_models)} models, {len(nn_features)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7556be44",
   "metadata": {},
   "source": [
    "## 2. Generate Simulated Odds\n",
    "\n",
    "Since we don't have historical odds data, we'll simulate realistic odds based on statistical distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97fb0736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features for test matches...\n",
      "Processing 2000 test matches...\n"
     ]
    }
   ],
   "source": [
    "# Extract features for test matches\n",
    "print(\"Extracting features for test matches...\")\n",
    "\n",
    "feature_extractor = TennisFeatureExtractor('tennis_data.db')\n",
    "\n",
    "# Get matches for feature extraction - use simpler approach\n",
    "# Use the same approach as in the model training notebooks\n",
    "test_query = \"\"\"\n",
    "    SELECT \n",
    "        m.match_id,\n",
    "        m.tournament_date,\n",
    "        m.surface,\n",
    "        m.winner_id,\n",
    "        m.loser_id,\n",
    "        m.best_of,\n",
    "        pw.player_name as winner_name,\n",
    "        pl.player_name as loser_name\n",
    "    FROM matches m\n",
    "    LEFT JOIN players pw ON m.winner_id = pw.player_id\n",
    "    LEFT JOIN players pl ON m.loser_id = pl.player_id\n",
    "    WHERE m.tournament_date >= '2023-01-01'\n",
    "        AND m.tournament_date < '2025-01-01'\n",
    "        AND m.surface IS NOT NULL\n",
    "    ORDER BY m.tournament_date\n",
    "    LIMIT 2000\n",
    "\"\"\"\n",
    "\n",
    "test_matches_subset = pd.read_sql_query(test_query, conn)\n",
    "print(f\"Processing {len(test_matches_subset)} test matches...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899be79d",
   "metadata": {},
   "source": [
    "## 3. Load Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c91aad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features for backtesting...\n",
      "  Processing 0/2000...\n",
      "  Processing 200/2000...\n",
      "  Processing 400/2000...\n",
      "  Processing 600/2000...\n",
      "  Processing 800/2000...\n",
      "  Processing 1000/2000...\n",
      "  Processing 1200/2000...\n",
      "  Processing 1400/2000...\n",
      "  Processing 1600/2000...\n",
      "  Processing 1800/2000...\n",
      "\n",
      "‚úÖ Extracted features for 2000 matches\n"
     ]
    }
   ],
   "source": [
    "# Extract features for each test match\n",
    "print(\"Extracting features for backtesting...\")\n",
    "\n",
    "feature_rows = []\n",
    "for idx, match in test_matches_subset.iterrows():\n",
    "    if idx % 200 == 0:\n",
    "        print(f\"  Processing {idx}/{len(test_matches_subset)}...\")\n",
    "    \n",
    "    try:\n",
    "        features = feature_extractor.extract_features(\n",
    "            match_id=match['match_id'],\n",
    "            match_date=match['tournament_date']\n",
    "        )\n",
    "        features['actual_winner'] = 1  # Winner is always player 1 by construction\n",
    "        feature_rows.append(features)\n",
    "    except Exception as e:\n",
    "        pass  # Skip matches with missing data\n",
    "\n",
    "df_test_features = pd.DataFrame(feature_rows)\n",
    "print(f\"\\n‚úÖ Extracted features for {len(df_test_features)} matches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c434f9c",
   "metadata": {},
   "source": [
    "## 4. Generate Predictions from All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8cec270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GENERATING PREDICTIONS FROM ALL MODELS\n",
      "============================================================\n",
      "\n",
      "1. Logistic Regression predictions...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m1. Logistic Regression predictions...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     30\u001b[39m X_lr = prepare_lr_features(df_test_features, lr_features)\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m lr_probs = \u001b[43mlr_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_lr\u001b[49m\u001b[43m)\u001b[49m[:, \u001b[32m1\u001b[39m]\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Mean prediction: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr_probs.mean()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     33\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Std: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr_probs.std()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tennis10/ml_models/logistic_regression.py:150\u001b[39m, in \u001b[36mSymmetricLogisticRegression.predict_proba\u001b[39m\u001b[34m(self, df)\u001b[39m\n\u001b[32m    147\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mModel not trained. Call fit() first.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    149\u001b[39m \u001b[38;5;66;03m# Prepare features\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m X = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mselected_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[38;5;66;03m# Standardize\u001b[39;00m\n\u001b[32m    153\u001b[39m X = \u001b[38;5;28mself\u001b[39m._standardize_features(X, \u001b[38;5;28mself\u001b[39m.selected_features, fit=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tennis10/ml_models/logistic_regression.py:63\u001b[39m, in \u001b[36mSymmetricLogisticRegression._prepare_features\u001b[39m\u001b[34m(self, df, features)\u001b[39m\n\u001b[32m     60\u001b[39m p1_col = \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mplayer1_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeat\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m     61\u001b[39m p2_col = \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mplayer2_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeat\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m p1_col \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m p2_col \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m df.columns:\n\u001b[32m     64\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing feature columns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp1_col\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m or \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp2_col\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m# Feature difference (player1 - player2)\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'numpy.ndarray' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "# Generate predictions from all models\n",
    "print(\"=\" * 60)\n",
    "print(\"GENERATING PREDICTIONS FROM ALL MODELS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get feature matrices\n",
    "def prepare_lr_features(df, feature_cols):\n",
    "    \"\"\"Prepare features for logistic regression.\"\"\"\n",
    "    X = df[feature_cols].values\n",
    "    return X\n",
    "\n",
    "def prepare_nn_features(df, feature_cols, scaler):\n",
    "    \"\"\"Prepare features for neural network.\"\"\"\n",
    "    X = df[feature_cols].values\n",
    "    X_scaled = scaler.transform(X)\n",
    "    return torch.FloatTensor(X_scaled)\n",
    "\n",
    "def ensemble_predict(models, X_tensor):\n",
    "    \"\"\"Get ensemble predictions.\"\"\"\n",
    "    all_preds = []\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            preds = model(X_tensor).numpy()\n",
    "            all_preds.append(preds)\n",
    "    return np.mean(all_preds, axis=0)\n",
    "\n",
    "# Logistic Regression predictions\n",
    "print(\"\\n1. Logistic Regression predictions...\")\n",
    "X_lr = prepare_lr_features(df_test_features, lr_features)\n",
    "lr_probs = lr_model.predict_proba(X_lr)[:, 1]\n",
    "print(f\"   Mean prediction: {lr_probs.mean():.3f}\")\n",
    "print(f\"   Std: {lr_probs.std():.3f}\")\n",
    "\n",
    "# Neural Network predictions\n",
    "print(\"\\n2. Neural Network predictions...\")\n",
    "X_nn = prepare_nn_features(df_test_features, nn_features, nn_scaler)\n",
    "nn_probs = ensemble_predict(nn_models, X_nn)\n",
    "print(f\"   Mean prediction: {nn_probs.mean():.3f}\")\n",
    "print(f\"   Std: {nn_probs.std():.3f}\")\n",
    "\n",
    "# Meta-Ensemble (weighted average)\n",
    "print(\"\\n3. Meta-Ensemble predictions...\")\n",
    "meta_weights = {'lr': 0.5, 'nn': 0.5}\n",
    "meta_probs = meta_weights['lr'] * lr_probs + meta_weights['nn'] * nn_probs\n",
    "print(f\"   Mean prediction: {meta_probs.mean():.3f}\")\n",
    "print(f\"   Std: {meta_probs.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f41cc58",
   "metadata": {},
   "source": [
    "## 5. Backtest Each Model with Kelly Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc9f347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtest all models\n",
    "initial_bankroll = 1000.0\n",
    "strategy = 'kelly'\n",
    "\n",
    "backtest_results = {}\n",
    "\n",
    "for model_name in ['markov', 'logistic', 'neural_net', 'ensemble']:\n",
    "    if model_name not in predictions_df.columns:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nBacktesting {model_name.upper()}...\")\n",
    "    \n",
    "    # Prepare predictions dataframe\n",
    "    model_preds = pd.DataFrame({\n",
    "        'match_id': predictions_df['match_id'],\n",
    "        'p_player1_win': predictions_df[model_name],\n",
    "        'actual_winner': predictions_df['actual_winner']\n",
    "    })\n",
    "    \n",
    "    # Backtest\n",
    "    result = backtest_model(\n",
    "        model_preds,\n",
    "        odds_df,\n",
    "        model_name=model_name,\n",
    "        strategy=strategy,\n",
    "        initial_bankroll=initial_bankroll\n",
    "    )\n",
    "    \n",
    "    backtest_results[model_name] = result\n",
    "    \n",
    "    print(f\"  ROI: {result['roi']:+.2%}\")\n",
    "    print(f\"  Final Bankroll: ${result['final_bankroll']:.2f}\")\n",
    "    print(f\"  Total Profit: ${result['total_profit']:+.2f}\")\n",
    "    print(f\"  Num Bets: {result['num_bets']}\")\n",
    "    print(f\"  Win Rate: {result['win_rate']:.2%}\")\n",
    "    print(f\"  Sharpe Ratio: {result['sharpe_ratio']:.2f}\")\n",
    "    print(f\"  Max Drawdown: {result['max_drawdown']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2405dfe",
   "metadata": {},
   "source": [
    "## 6. Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8b89a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "comparison_data = []\n",
    "\n",
    "for model_name, result in backtest_results.items():\n",
    "    comparison_data.append({\n",
    "        'Model': model_name.upper(),\n",
    "        'ROI': f\"{result['roi']:+.2%}\",\n",
    "        'Final Bankroll': f\"${result['final_bankroll']:.2f}\",\n",
    "        'Total Profit': f\"${result['total_profit']:+.2f}\",\n",
    "        'Num Bets': result['num_bets'],\n",
    "        'Win Rate': f\"{result['win_rate']:.2%}\",\n",
    "        'Avg Odds': f\"{result['avg_odds']:.2f}\",\n",
    "        'Sharpe Ratio': f\"{result['sharpe_ratio']:.2f}\",\n",
    "        'Max Drawdown': f\"{result['max_drawdown']:.2%}\"\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"MODEL COMPARISON - KELLY CRITERION STRATEGY\")\n",
    "print(\"=\" * 100)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be358fc",
   "metadata": {},
   "source": [
    "## 7. Bankroll Evolution Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48032e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot bankroll evolution for each model\n",
    "for model_name, result in backtest_results.items():\n",
    "    if len(result['bets_df']) > 0:\n",
    "        plot_bankroll_evolution(\n",
    "            result['bets_df'],\n",
    "            initial_bankroll=initial_bankroll,\n",
    "            title=f\"{model_name.upper()} - Bankroll Evolution (Kelly Criterion)\",\n",
    "            save_path=f\"backtesting_{model_name}_bankroll.png\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a538e6",
   "metadata": {},
   "source": [
    "## 8. Month-by-Month P&L Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fd04b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add dates to bets dataframes\n",
    "for model_name, result in backtest_results.items():\n",
    "    if len(result['bets_df']) > 0:\n",
    "        # Merge with test matches to get dates\n",
    "        bets_with_dates = result['bets_df'].merge(\n",
    "            test_matches[['match_id', 'tournament_date']],\n",
    "            on='match_id'\n",
    "        )\n",
    "        \n",
    "        # Plot monthly P&L\n",
    "        plot_monthly_pnl(\n",
    "            bets_with_dates,\n",
    "            date_column='tournament_date',\n",
    "            title=f\"{model_name.upper()} - Monthly P&L\",\n",
    "            save_path=f\"backtesting_{model_name}_monthly_pnl.png\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9226d7b9",
   "metadata": {},
   "source": [
    "## 9. Strategy Comparison (Best Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f61614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best model\n",
    "best_model = max(backtest_results.items(), key=lambda x: x[1]['roi'])\n",
    "best_model_name = best_model[0]\n",
    "\n",
    "print(f\"\\nBest Model: {best_model_name.upper()}\")\n",
    "print(f\"ROI: {best_model[1]['roi']:+.2%}\\n\")\n",
    "\n",
    "# Compare strategies for best model\n",
    "best_model_preds = pd.DataFrame({\n",
    "    'match_id': predictions_df['match_id'],\n",
    "    'p_player1_win': predictions_df[best_model_name],\n",
    "    'actual_winner': predictions_df['actual_winner']\n",
    "})\n",
    "\n",
    "strategy_comparison = compare_strategies(\n",
    "    best_model_preds,\n",
    "    odds_df,\n",
    "    strategies=['fixed', 'value', 'kelly'],\n",
    "    initial_bankroll=initial_bankroll\n",
    ")\n",
    "\n",
    "print(f\"Strategy Comparison for {best_model_name.upper()}:\")\n",
    "print(strategy_comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b0bb2f",
   "metadata": {},
   "source": [
    "## 10. Performance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c241e518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "model_names = list(backtest_results.keys())\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, len(model_names)))\n",
    "\n",
    "# Extract metrics\n",
    "rois = [backtest_results[m]['roi'] * 100 for m in model_names]\n",
    "sharpes = [backtest_results[m]['sharpe_ratio'] for m in model_names]\n",
    "drawdowns = [backtest_results[m]['max_drawdown'] * 100 for m in model_names]\n",
    "win_rates = [backtest_results[m]['win_rate'] * 100 for m in model_names]\n",
    "\n",
    "# Plot 1: ROI\n",
    "bars1 = axes[0, 0].bar([m.upper() for m in model_names], rois, color=colors, alpha=0.7, edgecolor='black')\n",
    "axes[0, 0].axhline(y=0, color='red', linestyle='--', linewidth=1)\n",
    "axes[0, 0].set_ylabel('ROI (%)', fontsize=12)\n",
    "axes[0, 0].set_title('Return on Investment', fontsize=13, fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "for bar, roi in zip(bars1, rois):\n",
    "    height = bar.get_height()\n",
    "    axes[0, 0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{roi:+.1f}%', ha='center', va='bottom' if roi >= 0 else 'top', fontweight='bold')\n",
    "\n",
    "# Plot 2: Sharpe Ratio\n",
    "bars2 = axes[0, 1].bar([m.upper() for m in model_names], sharpes, color=colors, alpha=0.7, edgecolor='black')\n",
    "axes[0, 1].set_ylabel('Sharpe Ratio', fontsize=12)\n",
    "axes[0, 1].set_title('Risk-Adjusted Returns', fontsize=13, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "for bar, sharpe in zip(bars2, sharpes):\n",
    "    height = bar.get_height()\n",
    "    axes[0, 1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{sharpe:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Plot 3: Max Drawdown\n",
    "bars3 = axes[1, 0].bar([m.upper() for m in model_names], drawdowns, color=colors, alpha=0.7, edgecolor='black')\n",
    "axes[1, 0].set_ylabel('Max Drawdown (%)', fontsize=12)\n",
    "axes[1, 0].set_title('Maximum Drawdown', fontsize=13, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "for bar, dd in zip(bars3, drawdowns):\n",
    "    height = bar.get_height()\n",
    "    axes[1, 0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{dd:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Plot 4: Win Rate\n",
    "bars4 = axes[1, 1].bar([m.upper() for m in model_names], win_rates, color=colors, alpha=0.7, edgecolor='black')\n",
    "axes[1, 1].axhline(y=50, color='red', linestyle='--', linewidth=1, label='Break-even')\n",
    "axes[1, 1].set_ylabel('Win Rate (%)', fontsize=12)\n",
    "axes[1, 1].set_title('Betting Win Rate', fontsize=13, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "axes[1, 1].legend()\n",
    "for bar, wr in zip(bars4, win_rates):\n",
    "    height = bar.get_height()\n",
    "    axes[1, 1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{wr:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('backtesting_model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Model comparison plot saved: backtesting_model_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780398bb",
   "metadata": {},
   "source": [
    "## 11. Final Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bc1a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"BACKTESTING SUMMARY REPORT\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"\\nüìÖ Report Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"\\nüìä Test Period: {test_matches['tournament_date'].min()} to {test_matches['tournament_date'].max()}\")\n",
    "print(f\"Test Matches: {len(test_matches):,}\")\n",
    "print(f\"\\nüí∞ Initial Bankroll: ${initial_bankroll:.2f}\")\n",
    "print(f\"Strategy: {strategy.upper()} (Kelly Criterion with 25% fractional sizing)\")\n",
    "print(f\"\\nüèÜ Best Performing Model: {best_model_name.upper()}\")\n",
    "print(f\"  ROI: {best_model[1]['roi']:+.2%}\")\n",
    "print(f\"  Final Bankroll: ${best_model[1]['final_bankroll']:.2f}\")\n",
    "print(f\"  Total Profit: ${best_model[1]['total_profit']:+.2f}\")\n",
    "print(f\"  Sharpe Ratio: {best_model[1]['sharpe_ratio']:.2f}\")\n",
    "print(f\"  Max Drawdown: {best_model[1]['max_drawdown']:.2%}\")\n",
    "print(f\"  Win Rate: {best_model[1]['win_rate']:.2%}\")\n",
    "print(f\"  Number of Bets: {best_model[1]['num_bets']}\")\n",
    "print(f\"\\nüìà All Models Performance:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(f\"\\nüìÅ Files Generated:\")\n",
    "for model_name in backtest_results.keys():\n",
    "    print(f\"  ‚úÖ backtesting_{model_name}_bankroll.png\")\n",
    "    print(f\"  ‚úÖ backtesting_{model_name}_monthly_pnl.png\")\n",
    "print(f\"  ‚úÖ backtesting_model_comparison.png\")\n",
    "print(\"\\n\" + \"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8ce80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close connections\n",
    "conn.close()\n",
    "feature_gen.close()\n",
    "markov_model.close()\n",
    "print(\"\\n‚úÖ Database connections closed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
