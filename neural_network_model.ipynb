{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2f1ac1a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m log_loss, brier_score_loss, accuracy_score\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcalibration\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m calibration_curve\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Add current directory to path\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import log_loss, brier_score_loss, accuracy_score\n",
    "from sklearn.calibration import calibration_curve\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "# Add current directory to path\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "# Import custom modules\n",
    "from features import TennisFeatureExtractor\n",
    "from ml_models.neural_network import (\n",
    "    SymmetricNeuralNetwork,\n",
    "    NeuralNetworkTrainer,\n",
    "    BaggingEnsemble\n",
    ")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "print(\"‚úÖ Libraries loaded\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Training started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb087c6",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14782e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to database\n",
    "conn = sqlite3.connect('tennis_data.db')\n",
    "\n",
    "# Load matches\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    m.match_id,\n",
    "    m.tournament_date,\n",
    "    m.surface,\n",
    "    m.winner_id,\n",
    "    m.loser_id,\n",
    "    m.best_of,\n",
    "    CASE WHEN m.winner_id < m.loser_id THEN 1 ELSE 2 END as winner\n",
    "FROM matches m\n",
    "WHERE m.tournament_date >= '2020-01-01'\n",
    "    AND m.tournament_date < '2025-01-01'\n",
    "    AND m.surface IS NOT NULL\n",
    "ORDER BY m.tournament_date\n",
    "\"\"\"\n",
    "\n",
    "matches = pd.read_sql_query(query, conn)\n",
    "\n",
    "print(f\"Total matches: {len(matches):,}\")\n",
    "print(f\"Date range: {matches['tournament_date'].min()} to {matches['tournament_date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ea66c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate features\n",
    "feature_gen = MatchFeatures('tennis_data.db')\n",
    "\n",
    "print(\"Generating features...\\n\")\n",
    "\n",
    "features_list = []\n",
    "\n",
    "for idx, match in matches.iterrows():\n",
    "    if idx % 500 == 0:\n",
    "        print(f\"Processing match {idx}/{len(matches)}...\")\n",
    "    \n",
    "    # Ensure player1_id < player2_id\n",
    "    if match['winner_id'] < match['loser_id']:\n",
    "        player1_id = match['winner_id']\n",
    "        player2_id = match['loser_id']\n",
    "    else:\n",
    "        player1_id = match['loser_id']\n",
    "        player2_id = match['winner_id']\n",
    "    \n",
    "    features = feature_gen.generate_features(\n",
    "        player1_id,\n",
    "        player2_id,\n",
    "        match['surface'],\n",
    "        match_date=match['tournament_date']\n",
    "    )\n",
    "    \n",
    "    features['match_id'] = match['match_id']\n",
    "    features['tournament_date'] = match['tournament_date']\n",
    "    features['winner'] = match['winner']\n",
    "    \n",
    "    features_list.append(features)\n",
    "\n",
    "df_features = pd.DataFrame(features_list)\n",
    "\n",
    "print(f\"\\n‚úÖ Generated features for {len(df_features)} matches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fb8030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "df_features['year'] = df_features['tournament_date'].str[:4]\n",
    "\n",
    "train_df = df_features[df_features['year'].isin(['2020', '2021'])].copy()\n",
    "val_df = df_features[df_features['year'] == '2022'].copy()\n",
    "test_df = df_features[df_features['year'].isin(['2023', '2024'])].copy()\n",
    "\n",
    "print(\"Data split:\")\n",
    "print(f\"  Training (2020-2021):   {len(train_df):,}\")\n",
    "print(f\"  Validation (2022):      {len(val_df):,}\")\n",
    "print(f\"  Test (2023-2024):       {len(test_df):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525f0804",
   "metadata": {},
   "source": [
    "## 2. Prepare Features (Exclude RANK, POINTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3bfa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names\n",
    "all_cols = df_features.columns.tolist()\n",
    "feature_cols = [col for col in all_cols if col.startswith('player1_')]\n",
    "feature_names = [col.replace('player1_', '') for col in feature_cols]\n",
    "\n",
    "# Exclude RANK and POINTS\n",
    "excluded_features = ['RANK', 'POINTS']\n",
    "available_features = [f for f in feature_names if f not in excluded_features]\n",
    "\n",
    "print(f\"Total features: {len(feature_names)}\")\n",
    "print(f\"Excluded: {excluded_features}\")\n",
    "print(f\"Available: {len(available_features)}\")\n",
    "print(f\"\\nFeatures: {available_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f8c61a",
   "metadata": {},
   "source": [
    "## 3. Train Single Neural Network (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bfc607",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training single neural network...\\n\")\n",
    "\n",
    "single_nn = NeuralNetworkTrainer(\n",
    "    n_features=len(available_features),\n",
    "    learning_rate=0.0004,\n",
    "    momentum=0.55,\n",
    "    weight_decay=0.002,\n",
    "    patience=10,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "single_nn.fit(train_df, val_df, available_features, max_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d78647d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate single model\n",
    "from sklearn.metrics import accuracy_score, log_loss, brier_score_loss\n",
    "\n",
    "test_pred_single = single_nn.predict(test_df, available_features)\n",
    "test_actuals = (test_df['winner'] == 1).astype(int).values\n",
    "\n",
    "single_accuracy = accuracy_score(test_actuals, test_pred_single.round())\n",
    "single_log_loss = log_loss(test_actuals, test_pred_single)\n",
    "single_brier = brier_score_loss(test_actuals, test_pred_single)\n",
    "\n",
    "print(\"\\nSingle Neural Network Performance:\")\n",
    "print(f\"  Accuracy:    {single_accuracy:.2%}\")\n",
    "print(f\"  Log Loss:    {single_log_loss:.4f}\")\n",
    "print(f\"  Brier Score: {single_brier:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79921626",
   "metadata": {},
   "source": [
    "## 4. Train Ensemble with Bagging (20 Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3292fa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ensemble of 20 neural networks\n",
    "ensemble_models, ensemble_stats = train_nn_ensemble(\n",
    "    train_df,\n",
    "    val_df,\n",
    "    available_features,\n",
    "    n_bags=20,\n",
    "    learning_rate=0.0004,\n",
    "    momentum=0.55,\n",
    "    weight_decay=0.002,\n",
    "    patience=10,\n",
    "    max_epochs=100,\n",
    "    verbose=False  # Set to False to reduce output\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15402853",
   "metadata": {},
   "source": [
    "## 5. Evaluate Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021a1e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble predictions\n",
    "test_pred_ensemble = predict_ensemble(ensemble_models, test_df, available_features)\n",
    "\n",
    "ensemble_accuracy = accuracy_score(test_actuals, test_pred_ensemble.round())\n",
    "ensemble_log_loss = log_loss(test_actuals, test_pred_ensemble)\n",
    "ensemble_brier = brier_score_loss(test_actuals, test_pred_ensemble)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ENSEMBLE VS SINGLE MODEL COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n{'Metric':<20} {'Single NN':<15} {'Ensemble (20)':<15} {'Improvement'}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Accuracy':<20} {single_accuracy:<15.2%} {ensemble_accuracy:<15.2%} {(ensemble_accuracy-single_accuracy)*100:+.2f}%\")\n",
    "print(f\"{'Log Loss':<20} {single_log_loss:<15.4f} {ensemble_log_loss:<15.4f} {(single_log_loss-ensemble_log_loss):+.4f}\")\n",
    "print(f\"{'Brier Score':<20} {single_brier:<15.4f} {ensemble_brier:<15.4f} {(single_brier-ensemble_brier):+.4f}\")\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6370747",
   "metadata": {},
   "source": [
    "## 6. Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8693321b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Single model learning curve\n",
    "single_history = single_nn.get_history()\n",
    "epochs_single = range(1, len(single_history['train_loss']) + 1)\n",
    "\n",
    "ax1.plot(epochs_single, single_history['train_loss'], 'b-', label='Training Loss', linewidth=2)\n",
    "ax1.plot(epochs_single, single_history['val_loss'], 'r-', label='Validation Loss', linewidth=2)\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Loss (Binary Cross-Entropy)', fontsize=12)\n",
    "ax1.set_title('Single Neural Network - Learning Curve', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Ensemble learning curves (all models)\n",
    "for i, (train_losses, val_losses) in enumerate(zip(ensemble_stats['train_losses'], \n",
    "                                                     ensemble_stats['val_losses'])):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    ax2.plot(epochs, train_losses, 'b-', alpha=0.2, linewidth=1)\n",
    "    ax2.plot(epochs, val_losses, 'r-', alpha=0.2, linewidth=1)\n",
    "\n",
    "# Average curves\n",
    "max_epochs = max([len(losses) for losses in ensemble_stats['train_losses']])\n",
    "avg_train = []\n",
    "avg_val = []\n",
    "for epoch in range(max_epochs):\n",
    "    train_at_epoch = [losses[epoch] for losses in ensemble_stats['train_losses'] \n",
    "                     if epoch < len(losses)]\n",
    "    val_at_epoch = [losses[epoch] for losses in ensemble_stats['val_losses'] \n",
    "                   if epoch < len(losses)]\n",
    "    avg_train.append(np.mean(train_at_epoch))\n",
    "    avg_val.append(np.mean(val_at_epoch))\n",
    "\n",
    "epochs_avg = range(1, len(avg_train) + 1)\n",
    "ax2.plot(epochs_avg, avg_train, 'b-', label='Avg Training Loss', linewidth=3)\n",
    "ax2.plot(epochs_avg, avg_val, 'r-', label='Avg Validation Loss', linewidth=3)\n",
    "\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Loss (Binary Cross-Entropy)', fontsize=12)\n",
    "ax2.set_title('Ensemble (20 Models) - Learning Curves', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('nn_learning_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Learning curves saved: nn_learning_curves.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4c7ccf",
   "metadata": {},
   "source": [
    "## 7. Feature Importance (Permutation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32cda1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate feature importance\n",
    "importance_df = calculate_permutation_importance(\n",
    "    ensemble_models,\n",
    "    val_df,\n",
    "    available_features,\n",
    "    n_repeats=5\n",
    ")\n",
    "\n",
    "print(\"\\nFeature Importance (Permutation):\")\n",
    "print(importance_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f0eaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "colors = plt.cm.RdYlGn(np.linspace(0.2, 0.8, len(importance_df)))\n",
    "bars = ax.barh(importance_df['feature'], importance_df['importance'], \n",
    "              color=colors, alpha=0.7, edgecolor='black')\n",
    "\n",
    "# Add error bars\n",
    "ax.errorbar(importance_df['importance'], importance_df['feature'],\n",
    "           xerr=importance_df['std'], fmt='none', ecolor='black', \n",
    "           capsize=3, alpha=0.5)\n",
    "\n",
    "ax.set_xlabel('Importance (Increase in Log-Loss)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Neural Network Feature Importance\\n(via Permutation)', \n",
    "            fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('nn_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Feature importance plot saved: nn_feature_importance.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febd036e",
   "metadata": {},
   "source": [
    "## 8. Prediction Distribution & Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3e5bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration analysis\n",
    "bins = np.linspace(0, 1, 11)\n",
    "bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "\n",
    "bin_indices = np.digitize(test_pred_ensemble, bins) - 1\n",
    "bin_indices = np.clip(bin_indices, 0, len(bin_centers) - 1)\n",
    "\n",
    "calibration_data = []\n",
    "for i in range(len(bin_centers)):\n",
    "    mask = bin_indices == i\n",
    "    if mask.sum() > 0:\n",
    "        actual_rate = test_actuals[mask].mean()\n",
    "        predicted_rate = test_pred_ensemble[mask].mean()\n",
    "        count = mask.sum()\n",
    "        calibration_data.append({\n",
    "            'predicted': predicted_rate,\n",
    "            'actual': actual_rate,\n",
    "            'count': count\n",
    "        })\n",
    "\n",
    "calib_df = pd.DataFrame(calibration_data)\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Calibration curve\n",
    "ax1.plot([0, 1], [0, 1], 'r--', linewidth=2, label='Perfect Calibration')\n",
    "ax1.scatter(calib_df['predicted'], calib_df['actual'], \n",
    "           s=calib_df['count']*2, alpha=0.6, color='blue')\n",
    "ax1.plot(calib_df['predicted'], calib_df['actual'], 'b-', linewidth=1, alpha=0.5)\n",
    "\n",
    "ax1.set_xlabel('Predicted Probability', fontsize=12)\n",
    "ax1.set_ylabel('Actual Win Rate', fontsize=12)\n",
    "ax1.set_title('Calibration Curve\\n(size = number of matches)', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xlim(0, 1)\n",
    "ax1.set_ylim(0, 1)\n",
    "\n",
    "# Prediction distribution\n",
    "ax2.hist(test_pred_ensemble, bins=30, alpha=0.7, color='blue', edgecolor='black')\n",
    "ax2.axvline(x=0.5, color='red', linestyle='--', linewidth=2, label='50% threshold')\n",
    "ax2.set_xlabel('Predicted Probability', fontsize=12)\n",
    "ax2.set_ylabel('Frequency', fontsize=12)\n",
    "ax2.set_title('Ensemble Prediction Distribution', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('nn_calibration.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Calibration plot saved: nn_calibration.png\")\n",
    "\n",
    "calib_error = np.abs(calib_df['predicted'] - calib_df['actual']).mean()\n",
    "print(f\"\\nMean Calibration Error: {calib_error:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77e92a0",
   "metadata": {},
   "source": [
    "## 9. Performance by Surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca014e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get surface information\n",
    "test_with_surface = test_df.merge(matches[['match_id', 'surface']], on='match_id')\n",
    "\n",
    "surface_results = []\n",
    "\n",
    "for surface in ['Hard', 'Clay', 'Grass']:\n",
    "    mask = test_with_surface['surface'] == surface\n",
    "    if mask.sum() == 0:\n",
    "        continue\n",
    "    \n",
    "    surface_probs = test_pred_ensemble[mask]\n",
    "    surface_actuals = test_actuals[mask]\n",
    "    \n",
    "    surface_results.append({\n",
    "        'Surface': surface,\n",
    "        'Matches': mask.sum(),\n",
    "        'Accuracy': accuracy_score(surface_actuals, surface_probs.round()),\n",
    "        'Log Loss': log_loss(surface_actuals, surface_probs)\n",
    "    })\n",
    "\n",
    "surface_df = pd.DataFrame(surface_results)\n",
    "\n",
    "print(\"\\nPerformance by Surface:\")\n",
    "print(surface_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d936f719",
   "metadata": {},
   "source": [
    "## 10. Save Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63db9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save ensemble\n",
    "ensemble_data = {\n",
    "    'models': ensemble_models,\n",
    "    'features': available_features,\n",
    "    'ensemble_stats': ensemble_stats,\n",
    "    'test_metrics': {\n",
    "        'accuracy': ensemble_accuracy,\n",
    "        'log_loss': ensemble_log_loss,\n",
    "        'brier_score': ensemble_brier,\n",
    "        'calibration_error': calib_error\n",
    "    },\n",
    "    'feature_importance': importance_df\n",
    "}\n",
    "\n",
    "with open('ml_models/nn_ensemble.pkl', 'wb') as f:\n",
    "    pickle.dump(ensemble_data, f)\n",
    "\n",
    "print(\"‚úÖ Ensemble saved: ml_models/nn_ensemble.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d3740a",
   "metadata": {},
   "source": [
    "## 11. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92667e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"NEURAL NETWORK ENSEMBLE - FINAL REPORT\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nüìÖ Training completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"\\nüèóÔ∏è  Architecture:\")\n",
    "print(f\"  Input features: {len(available_features)}\")\n",
    "print(f\"  Hidden neurons: 100 (tanh activation)\")\n",
    "print(f\"  Output: 1 (sigmoid activation)\")\n",
    "print(f\"  Bias terms: None (symmetric design)\")\n",
    "print(f\"\\nüéØ Training Configuration:\")\n",
    "print(f\"  Optimizer: SGD (momentum=0.55)\")\n",
    "print(f\"  Learning rate: 0.0004\")\n",
    "print(f\"  Weight decay: 0.002\")\n",
    "print(f\"  Batch size: 1 (online learning)\")\n",
    "print(f\"  Early stopping: patience=10\")\n",
    "print(f\"\\nüé≤ Ensemble:\")\n",
    "print(f\"  Number of models: {len(ensemble_models)}\")\n",
    "print(f\"  Bagging: Bootstrap sampling\")\n",
    "print(f\"  Prediction: Average of all models\")\n",
    "print(f\"\\nüìä Test Set Performance (2023-2024):\")\n",
    "print(f\"  Test samples: {len(test_df):,}\")\n",
    "print(f\"  Accuracy:      {ensemble_accuracy:.2%}\")\n",
    "print(f\"  Log Loss:      {ensemble_log_loss:.4f}\")\n",
    "print(f\"  Brier Score:   {ensemble_brier:.4f}\")\n",
    "print(f\"  Calibration:   {calib_error:.4f}\")\n",
    "print(f\"\\nüìà Improvement over Single Model:\")\n",
    "print(f\"  Accuracy:      {(ensemble_accuracy-single_accuracy)*100:+.2f}%\")\n",
    "print(f\"  Log Loss:      {(single_log_loss-ensemble_log_loss):+.4f}\")\n",
    "print(f\"  Brier Score:   {(single_brier-ensemble_brier):+.4f}\")\n",
    "print(f\"\\nüìÅ Files Generated:\")\n",
    "print(f\"  ‚úÖ nn_learning_curves.png\")\n",
    "print(f\"  ‚úÖ nn_feature_importance.png\")\n",
    "print(f\"  ‚úÖ nn_calibration.png\")\n",
    "print(f\"  ‚úÖ ml_models/nn_ensemble.pkl\")\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44ede0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close connections\n",
    "conn.close()\n",
    "feature_gen.close()\n",
    "print(\"\\n‚úÖ Database connections closed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
