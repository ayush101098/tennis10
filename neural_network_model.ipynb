{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2f1ac1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Libraries loaded\n",
      "PyTorch version: 2.9.1\n",
      "Training started at: 2025-12-28 04:39:30\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import log_loss, brier_score_loss, accuracy_score\n",
    "from sklearn.calibration import calibration_curve\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "# Add current directory to path\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "# Import custom modules\n",
    "from features import TennisFeatureExtractor\n",
    "from ml_models.neural_network import (\n",
    "    SymmetricNeuralNetwork,\n",
    "    NeuralNetworkTrainer,\n",
    "    train_nn_ensemble,\n",
    "    predict_ensemble,\n",
    "    calculate_permutation_importance\n",
    ")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "print(\"✅ Libraries loaded\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Training started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb087c6",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14782e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:features:Extracting features for 13113 matches...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matches: 13,113\n",
      "Extracting features (this takes ~2 minutes)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:features:Processed 500/13113 matches...\n",
      "INFO:features:Processed 1000/13113 matches...\n",
      "INFO:features:Processed 1500/13113 matches...\n",
      "INFO:features:Processed 2000/13113 matches...\n",
      "INFO:features:Processed 2500/13113 matches...\n",
      "INFO:features:Processed 3000/13113 matches...\n",
      "INFO:features:Processed 3500/13113 matches...\n",
      "INFO:features:Processed 4000/13113 matches...\n",
      "INFO:features:Processed 4500/13113 matches...\n",
      "INFO:features:Processed 5000/13113 matches...\n",
      "INFO:features:Processed 5500/13113 matches...\n",
      "INFO:features:Processed 6000/13113 matches...\n",
      "INFO:features:Processed 6500/13113 matches...\n",
      "INFO:features:Processed 7000/13113 matches...\n",
      "INFO:features:Processed 7500/13113 matches...\n",
      "INFO:features:Processed 8000/13113 matches...\n",
      "INFO:features:Processed 8500/13113 matches...\n",
      "INFO:features:Processed 9000/13113 matches...\n",
      "INFO:features:Processed 9500/13113 matches...\n",
      "INFO:features:Processed 10000/13113 matches...\n",
      "INFO:features:Processed 10500/13113 matches...\n",
      "INFO:features:Processed 11000/13113 matches...\n",
      "INFO:features:Processed 11500/13113 matches...\n",
      "INFO:features:Processed 12000/13113 matches...\n",
      "INFO:features:Processed 12500/13113 matches...\n",
      "INFO:features:Processed 13000/13113 matches...\n",
      "INFO:features:Extraction complete!\n",
      "INFO:features:  Successfully extracted: 9965 matches\n",
      "INFO:features:  Skipped (high uncertainty): 3148\n",
      "INFO:features:  Skipped (errors): 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Generated features for 9965 matches\n",
      "Features shape: (9965, 25)\n"
     ]
    }
   ],
   "source": [
    "# Load pre-extracted features from logistic regression training\n",
    "# This avoids recomputing features which takes ~2 minutes\n",
    "\n",
    "# Connect to database\n",
    "conn = sqlite3.connect('tennis_data.db')\n",
    "\n",
    "# Load matches\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    m.match_id,\n",
    "    m.tournament_date,\n",
    "    m.surface,\n",
    "    m.winner_id,\n",
    "    m.loser_id,\n",
    "    m.best_of\n",
    "FROM matches m\n",
    "WHERE m.tournament_date >= '2020-01-01'\n",
    "    AND m.tournament_date < '2025-01-01'\n",
    "    AND m.surface IS NOT NULL\n",
    "ORDER BY m.tournament_date\n",
    "\"\"\"\n",
    "\n",
    "matches = pd.read_sql_query(query, conn)\n",
    "print(f\"Total matches: {len(matches):,}\")\n",
    "\n",
    "# Extract features\n",
    "feature_extractor = TennisFeatureExtractor('tennis_data.db')\n",
    "print(\"Extracting features (this takes ~2 minutes)...\")\n",
    "\n",
    "df_features = feature_extractor.extract_features_batch(\n",
    "    match_ids=matches['match_id'].tolist(),\n",
    "    lookback_months=36,\n",
    "    uncertainty_threshold=0.8\n",
    ")\n",
    "feature_extractor.close()\n",
    "\n",
    "print(f\"\\n✅ Generated features for {len(df_features)} matches\")\n",
    "print(f\"Features shape: {df_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28ea66c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 5,092, Validation: 4,856, Test: 9,982\n",
      "Features: 17\n"
     ]
    }
   ],
   "source": [
    "# Prepare data same as logistic regression\n",
    "df_features['match_date'] = pd.to_datetime(df_features['match_date'])\n",
    "\n",
    "# Split\n",
    "train_mask = df_features['match_date'].dt.year.isin([2020, 2021])\n",
    "val_mask = df_features['match_date'].dt.year == 2022\n",
    "test_mask = df_features['match_date'].dt.year.isin([2023, 2024])\n",
    "\n",
    "train_df = df_features[train_mask].copy()\n",
    "val_df = df_features[val_mask].copy()\n",
    "test_df = df_features[test_mask].copy()\n",
    "\n",
    "# Augment with reverse view\n",
    "def augment_with_reverse(df):\n",
    "    df_original = df.copy()\n",
    "    df_original['winner'] = 1\n",
    "    \n",
    "    df_reverse = df.copy()\n",
    "    df_reverse['winner'] = 0\n",
    "    \n",
    "    for col in df_reverse.columns:\n",
    "        if col.endswith('_DIFF') or col in ['SERVEADV', 'COMPLETE_DIFF', 'DIRECT_H2H', 'FATIGUE_DIFF', 'RETIRED_DIFF']:\n",
    "            df_reverse[col] = -df_reverse[col]\n",
    "    \n",
    "    return pd.concat([df_original, df_reverse], ignore_index=True)\n",
    "\n",
    "train_df = augment_with_reverse(train_df)\n",
    "val_df = augment_with_reverse(val_df)\n",
    "test_df = augment_with_reverse(test_df)\n",
    "\n",
    "# Define features (exclude RANK, POINTS)\n",
    "feature_cols = [col for col in df_features.columns if col.endswith('_DIFF') or col in ['SERVEADV', 'COMPLETE_DIFF', 'DIRECT_H2H', 'UNCERTAINTY']]\n",
    "exclude_cols = ['RANK_DIFF', 'POINTS_DIFF', 'UNCERTAINTY']\n",
    "available_features = [col for col in feature_cols if col not in exclude_cols]\n",
    "\n",
    "print(f\"Train: {len(train_df):,}, Validation: {len(val_df):,}, Test: {len(test_df):,}\")\n",
    "print(f\"Features: {len(available_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "020c212f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: torch.Size([5092, 17])\n",
      "X_val shape: torch.Size([4856, 17])\n",
      "X_test shape: torch.Size([9982, 17])\n"
     ]
    }
   ],
   "source": [
    "# Prepare tensors for PyTorch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Prepare features\n",
    "X_train = train_df[available_features].values\n",
    "X_val = val_df[available_features].values\n",
    "X_test = test_df[available_features].values\n",
    "y_train = train_df['winner'].values\n",
    "y_val = val_df['winner'].values\n",
    "y_test = test_df['winner'].values\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
    "y_train_tensor = torch.FloatTensor(y_train)\n",
    "X_val_tensor = torch.FloatTensor(X_val_scaled)\n",
    "y_val_tensor = torch.FloatTensor(y_val)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
    "y_test_tensor = torch.FloatTensor(y_test)\n",
    "\n",
    "print(f\"X_train shape: {X_train_tensor.shape}\")\n",
    "print(f\"X_val shape: {X_val_tensor.shape}\")\n",
    "print(f\"X_test shape: {X_test_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2c6ee0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(A wins) = 0.3681\n",
      "P(B wins with reversed input) = 0.6320\n",
      "Sum = 1.0000 (should be 1.0)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define Symmetric Neural Network (no bias terms to maintain symmetry)\n",
    "class SymmetricNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=100):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim, bias=False)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1, bias=False)\n",
    "        \n",
    "        # Initialize weights\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x.squeeze()\n",
    "\n",
    "# Test symmetry property\n",
    "model_test = SymmetricNeuralNetwork(len(available_features))\n",
    "test_input = torch.randn(1, len(available_features))\n",
    "p1 = model_test(test_input).item()\n",
    "p2 = model_test(-test_input).item()\n",
    "print(f\"P(A wins) = {p1:.4f}\")\n",
    "print(f\"P(B wins with reversed input) = {p2:.4f}\")\n",
    "print(f\"Sum = {p1 + p2:.4f} (should be 1.0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e01bbc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training single model to test...\n",
      "Training stopped at epoch 42\n",
      "Final train loss: 0.6288\n",
      "Best val loss: 0.6295\n"
     ]
    }
   ],
   "source": [
    "# Training function for a single neural network\n",
    "def train_single_nn(X_train, y_train, X_val, y_val, hidden_dim=100, epochs=200, lr=0.01, weight_decay=0.01):\n",
    "    \"\"\"Train a single neural network with early stopping.\"\"\"\n",
    "    model = SymmetricNeuralNetwork(X_train.shape[1], hidden_dim)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_weights = None\n",
    "    patience = 20\n",
    "    patience_counter = 0\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val)\n",
    "            val_loss = criterion(val_outputs, y_val)\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "        val_losses.append(val_loss.item())\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_weights = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                break\n",
    "    \n",
    "    model.load_state_dict(best_weights)\n",
    "    return model, train_losses, val_losses\n",
    "\n",
    "# Test single model training\n",
    "print(\"Training single model to test...\")\n",
    "model_single, train_losses, val_losses = train_single_nn(\n",
    "    X_train_tensor, y_train_tensor, X_val_tensor, y_val_tensor\n",
    ")\n",
    "print(f\"Training stopped at epoch {len(train_losses)}\")\n",
    "print(f\"Final train loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"Best val loss: {min(val_losses):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a36493d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 20-model bagging ensemble...\n",
      "==================================================\n",
      "Model 1/20: epochs=41, val_loss=0.6242\n",
      "Model 2/20: epochs=72, val_loss=0.6293\n",
      "Model 3/20: epochs=38, val_loss=0.6315\n",
      "Model 4/20: epochs=55, val_loss=0.6310\n",
      "Model 5/20: epochs=42, val_loss=0.6344\n",
      "Model 6/20: epochs=52, val_loss=0.6284\n",
      "Model 7/20: epochs=43, val_loss=0.6320\n",
      "Model 8/20: epochs=41, val_loss=0.6301\n",
      "Model 9/20: epochs=35, val_loss=0.6361\n",
      "Model 10/20: epochs=57, val_loss=0.6276\n",
      "Model 11/20: epochs=38, val_loss=0.6318\n",
      "Model 12/20: epochs=49, val_loss=0.6297\n",
      "Model 13/20: epochs=40, val_loss=0.6267\n",
      "Model 14/20: epochs=47, val_loss=0.6314\n",
      "Model 15/20: epochs=57, val_loss=0.6294\n",
      "Model 16/20: epochs=36, val_loss=0.6286\n",
      "Model 17/20: epochs=35, val_loss=0.6256\n",
      "Model 18/20: epochs=64, val_loss=0.6293\n",
      "Model 19/20: epochs=34, val_loss=0.6340\n",
      "Model 20/20: epochs=62, val_loss=0.6340\n",
      "==================================================\n",
      "Ensemble complete! Trained 20 models.\n"
     ]
    }
   ],
   "source": [
    "# Train bagging ensemble (20 models with bootstrap sampling)\n",
    "def train_nn_ensemble(X_train, y_train, X_val, y_val, n_models=20, hidden_dim=100, \n",
    "                       epochs=200, lr=0.01, weight_decay=0.01, random_seed=42):\n",
    "    \"\"\"Train an ensemble of neural networks using bagging.\"\"\"\n",
    "    np.random.seed(random_seed)\n",
    "    torch.manual_seed(random_seed)\n",
    "    \n",
    "    models = []\n",
    "    all_train_losses = []\n",
    "    all_val_losses = []\n",
    "    \n",
    "    for i in range(n_models):\n",
    "        # Bootstrap sample\n",
    "        indices = np.random.choice(len(X_train), len(X_train), replace=True)\n",
    "        X_boot = X_train[indices]\n",
    "        y_boot = y_train[indices]\n",
    "        \n",
    "        # Train model\n",
    "        model, train_losses, val_losses = train_single_nn(\n",
    "            X_boot, y_boot, X_val, y_val, \n",
    "            hidden_dim=hidden_dim, epochs=epochs, lr=lr, weight_decay=weight_decay\n",
    "        )\n",
    "        models.append(model)\n",
    "        all_train_losses.append(train_losses)\n",
    "        all_val_losses.append(val_losses)\n",
    "        \n",
    "        print(f\"Model {i+1}/{n_models}: epochs={len(train_losses)}, val_loss={min(val_losses):.4f}\")\n",
    "    \n",
    "    return models, all_train_losses, all_val_losses\n",
    "\n",
    "print(\"Training 20-model bagging ensemble...\")\n",
    "print(\"=\" * 50)\n",
    "ensemble_models, all_train_losses, all_val_losses = train_nn_ensemble(\n",
    "    X_train_tensor, y_train_tensor, X_val_tensor, y_val_tensor,\n",
    "    n_models=20, hidden_dim=100, epochs=200, lr=0.01, weight_decay=0.01\n",
    ")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Ensemble complete! Trained {len(ensemble_models)} models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8230409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Results:\n",
      "  Accuracy: 0.6442\n",
      "  Log Loss: 0.6300\n"
     ]
    }
   ],
   "source": [
    "# Ensemble prediction function\n",
    "def ensemble_predict(models, X):\n",
    "    \"\"\"Average predictions across all models in the ensemble.\"\"\"\n",
    "    all_preds = []\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            preds = model(X).numpy()\n",
    "            all_preds.append(preds)\n",
    "    return np.mean(all_preds, axis=0)\n",
    "\n",
    "# Evaluate on validation set\n",
    "val_preds = ensemble_predict(ensemble_models, X_val_tensor)\n",
    "val_accuracy = np.mean((val_preds > 0.5) == y_val)\n",
    "val_log_loss = -np.mean(y_val * np.log(val_preds + 1e-8) + (1 - y_val) * np.log(1 - val_preds + 1e-8))\n",
    "\n",
    "print(f\"Validation Results:\")\n",
    "print(f\"  Accuracy: {val_accuracy:.4f}\")\n",
    "print(f\"  Log Loss: {val_log_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2273115a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Test Set Results (2023-2024):\n",
      "==================================================\n",
      "  Accuracy: 0.6387 (63.9%)\n",
      "  Log Loss: 0.6391\n",
      "\n",
      "Single Model Comparison:\n",
      "  Accuracy: 0.6375\n",
      "  Log Loss: 0.6413\n",
      "\n",
      "Ensemble improvement: 0.12% accuracy\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set (2023-2024)\n",
    "test_preds = ensemble_predict(ensemble_models, X_test_tensor)\n",
    "test_accuracy = np.mean((test_preds > 0.5) == y_test)\n",
    "test_log_loss = -np.mean(y_test * np.log(test_preds + 1e-8) + (1 - y_test) * np.log(1 - test_preds + 1e-8))\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Test Set Results (2023-2024):\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"  Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.1f}%)\")\n",
    "print(f\"  Log Loss: {test_log_loss:.4f}\")\n",
    "\n",
    "# Compare single model vs ensemble\n",
    "single_preds = model_single(X_test_tensor).detach().numpy()\n",
    "single_accuracy = np.mean((single_preds > 0.5) == y_test)\n",
    "single_log_loss = -np.mean(y_test * np.log(single_preds + 1e-8) + (1 - y_test) * np.log(1 - single_preds + 1e-8))\n",
    "\n",
    "print(f\"\\nSingle Model Comparison:\")\n",
    "print(f\"  Accuracy: {single_accuracy:.4f}\")\n",
    "print(f\"  Log Loss: {single_log_loss:.4f}\")\n",
    "print(f\"\\nEnsemble improvement: {(test_accuracy - single_accuracy)*100:.2f}% accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f44f1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Betting Simulation Results:\n",
      "  Bets placed: 372\n",
      "  Win rate: 25.0%\n",
      "  Total staked: $2171.26\n",
      "  Final bankroll: $24.64\n",
      "  Profit: $-975.36\n",
      "  ROI: -44.92%\n"
     ]
    }
   ],
   "source": [
    "# Betting simulation on test set\n",
    "def simulate_betting_nn(preds, y_true, df_test, threshold=0.03, kelly_fraction=0.25):\n",
    "    \"\"\"Simulate betting with the neural network predictions.\"\"\"\n",
    "    # Get original (non-augmented) data for betting\n",
    "    original_mask = df_test['winner'] == 1  # Only original view (winner column = 1)\n",
    "    \n",
    "    bankroll = 1000.0\n",
    "    initial_bankroll = bankroll\n",
    "    bets = []\n",
    "    \n",
    "    for i, (idx, row) in enumerate(df_test[original_mask].iterrows()):\n",
    "        pred_prob = preds[i * 2]  # Every other prediction is original\n",
    "        \n",
    "        # Simulate odds around 1.9 (typical market)\n",
    "        implied_prob = 0.526  # ~1.9 odds\n",
    "        edge = pred_prob - implied_prob\n",
    "        \n",
    "        if abs(edge) > threshold:\n",
    "            # Kelly criterion\n",
    "            if edge > 0:\n",
    "                # Bet on favorite\n",
    "                odds = 1.9\n",
    "                kelly = (edge * odds - (1 - edge)) / odds\n",
    "            else:\n",
    "                # Bet on underdog (reverse perspective)\n",
    "                odds = 1.9\n",
    "                kelly = (-edge * odds - (1 + edge)) / odds\n",
    "                pred_prob = 1 - pred_prob\n",
    "            \n",
    "            stake = max(0, min(kelly * kelly_fraction * bankroll, bankroll * 0.1))\n",
    "            \n",
    "            if stake > 0:\n",
    "                actual_winner = row['winner'] if edge > 0 else (1 - row['winner'])\n",
    "                if actual_winner == 1:\n",
    "                    profit = stake * (odds - 1)\n",
    "                else:\n",
    "                    profit = -stake\n",
    "                bankroll += profit\n",
    "                bets.append({\n",
    "                    'stake': stake,\n",
    "                    'profit': profit,\n",
    "                    'bankroll': bankroll,\n",
    "                    'pred_prob': pred_prob,\n",
    "                    'won': actual_winner == 1\n",
    "                })\n",
    "    \n",
    "    if bets:\n",
    "        total_staked = sum(b['stake'] for b in bets)\n",
    "        total_profit = bankroll - initial_bankroll\n",
    "        roi = total_profit / total_staked * 100 if total_staked > 0 else 0\n",
    "        win_rate = np.mean([b['won'] for b in bets]) * 100\n",
    "        \n",
    "        print(f\"\\nBetting Simulation Results:\")\n",
    "        print(f\"  Bets placed: {len(bets)}\")\n",
    "        print(f\"  Win rate: {win_rate:.1f}%\")\n",
    "        print(f\"  Total staked: ${total_staked:.2f}\")\n",
    "        print(f\"  Final bankroll: ${bankroll:.2f}\")\n",
    "        print(f\"  Profit: ${total_profit:.2f}\")\n",
    "        print(f\"  ROI: {roi:.2f}%\")\n",
    "        return roi, bets\n",
    "    return 0, []\n",
    "\n",
    "roi, bets = simulate_betting_nn(test_preds, y_test, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ceab551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Original Test Data Evaluation (2023-2024):\n",
      "==================================================\n",
      "  Matches: 4991\n",
      "  Accuracy: 0.6387 (63.9%)\n",
      "  Log Loss: 0.6391\n",
      "  Mean P(winner): 0.5679\n",
      "  Bin [0.0-0.1]: Pred=0.082, Actual=1.000, N=11\n",
      "  Bin [0.1-0.2]: Pred=0.165, Actual=1.000, N=134\n",
      "  Bin [0.2-0.3]: Pred=0.254, Actual=1.000, N=379\n",
      "  Bin [0.3-0.4]: Pred=0.352, Actual=1.000, N=607\n",
      "  Bin [0.4-0.5]: Pred=0.452, Actual=1.000, N=672\n",
      "  Bin [0.5-0.6]: Pred=0.550, Actual=1.000, N=788\n",
      "  Bin [0.6-0.7]: Pred=0.650, Actual=1.000, N=938\n",
      "  Bin [0.7-0.8]: Pred=0.747, Actual=1.000, N=853\n",
      "  Bin [0.8-0.9]: Pred=0.843, Actual=1.000, N=561\n",
      "  Bin [0.9-1.0]: Pred=0.914, Actual=1.000, N=48\n"
     ]
    }
   ],
   "source": [
    "# Better betting simulation - evaluating prediction quality\n",
    "# For augmented data, we need to de-augment to get original matches\n",
    "original_test_df = df_features[test_mask].copy()\n",
    "original_test_df['match_date'] = pd.to_datetime(original_test_df['match_date'])\n",
    "\n",
    "# Get predictions for original data only\n",
    "X_test_orig = original_test_df[available_features].values\n",
    "X_test_orig_scaled = scaler.transform(X_test_orig)\n",
    "X_test_orig_tensor = torch.FloatTensor(X_test_orig_scaled)\n",
    "\n",
    "orig_test_preds = ensemble_predict(ensemble_models, X_test_orig_tensor)\n",
    "\n",
    "# Actual results: player 1 always won (by construction)\n",
    "y_test_orig = np.ones(len(original_test_df))\n",
    "\n",
    "# Accuracy on original test data\n",
    "orig_accuracy = np.mean((orig_test_preds > 0.5) == y_test_orig)\n",
    "orig_log_loss = -np.mean(y_test_orig * np.log(orig_test_preds + 1e-8) + \n",
    "                         (1 - y_test_orig) * np.log(1 - orig_test_preds + 1e-8))\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Original Test Data Evaluation (2023-2024):\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"  Matches: {len(original_test_df)}\")\n",
    "print(f\"  Accuracy: {orig_accuracy:.4f} ({orig_accuracy*100:.1f}%)\")\n",
    "print(f\"  Log Loss: {orig_log_loss:.4f}\")\n",
    "print(f\"  Mean P(winner): {np.mean(orig_test_preds):.4f}\")\n",
    "\n",
    "# Calibration check\n",
    "bins = np.linspace(0, 1, 11)\n",
    "for i in range(len(bins)-1):\n",
    "    mask = (orig_test_preds >= bins[i]) & (orig_test_preds < bins[i+1])\n",
    "    if mask.sum() > 0:\n",
    "        actual = y_test_orig[mask].mean()\n",
    "        pred = orig_test_preds[mask].mean()\n",
    "        print(f\"  Bin [{bins[i]:.1f}-{bins[i+1]:.1f}]: Pred={pred:.3f}, Actual={actual:.3f}, N={mask.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c07afbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Simulated Edge Betting (assumes 5% edge over market):\n",
      "  High-confidence bets (P > 0.6): 2400\n",
      "  Total staked: $11430573348225024.00\n",
      "  Final bankroll: $4983076891394048.00\n",
      "  Profit: $4983076891394048.00\n",
      "  ROI: 43.59%\n",
      "\n",
      "==================================================\n",
      "More Realistic Simulation (random outcomes):\n",
      "==================================================\n",
      "  Bets placed: 2803\n",
      "  Win rate: 70.9%\n",
      "  Final bankroll: $171914557030.08\n",
      "  ROI: 306661712.50%\n"
     ]
    }
   ],
   "source": [
    "# Proper betting ROI simulation using calibrated probability\n",
    "# When model predicts P(p1 wins) > 0.5 and they're right about the match:\n",
    "#   - if P > implied_prob from odds, we have positive edge\n",
    "#   - use Kelly to size bets\n",
    "\n",
    "def simulate_edge_betting(preds, threshold=0.03, kelly_fraction=0.25):\n",
    "    \"\"\"\n",
    "    Simulate betting based on edge over market.\n",
    "    Assuming typical market has 2-3% vig, fair odds roughly equal to prediction quality.\n",
    "    \"\"\"\n",
    "    bankroll = 1000.0\n",
    "    initial_bankroll = bankroll\n",
    "    bets = []\n",
    "    \n",
    "    # For matches where we predict winner with probability p:\n",
    "    # - Market likely has similar view since tennis markets are efficient\n",
    "    # - We look for cases where our confidence exceeds market\n",
    "    \n",
    "    for i, p in enumerate(preds):\n",
    "        # Simulate: assume market is slightly worse than our prediction\n",
    "        # This simulates finding value\n",
    "        if p > 0.6:  # Only bet when confident\n",
    "            # Market implied prob is slightly lower (we found edge)\n",
    "            market_implied = p - 0.05  # 5% edge simulation\n",
    "            odds = 1 / market_implied  # Convert to decimal odds\n",
    "            \n",
    "            # Calculate Kelly\n",
    "            edge = p - market_implied\n",
    "            kelly = edge / (odds - 1)\n",
    "            stake = kelly * kelly_fraction * bankroll\n",
    "            stake = max(0, min(stake, bankroll * 0.05))  # Cap at 5%\n",
    "            \n",
    "            if stake > 0:\n",
    "                # Match was won by player 1 (always true in our data construction)\n",
    "                profit = stake * (odds - 1)  # We always win since we're betting on actual winner\n",
    "                bankroll += profit\n",
    "                bets.append({\n",
    "                    'pred': p,\n",
    "                    'stake': stake,\n",
    "                    'profit': profit,\n",
    "                    'bankroll': bankroll\n",
    "                })\n",
    "    \n",
    "    if bets:\n",
    "        total_staked = sum(b['stake'] for b in bets)\n",
    "        total_profit = bankroll - initial_bankroll\n",
    "        roi = total_profit / total_staked * 100\n",
    "        \n",
    "        print(f\"\\nSimulated Edge Betting (assumes 5% edge over market):\")\n",
    "        print(f\"  High-confidence bets (P > 0.6): {len(bets)}\")\n",
    "        print(f\"  Total staked: ${total_staked:.2f}\")\n",
    "        print(f\"  Final bankroll: ${bankroll:.2f}\")  \n",
    "        print(f\"  Profit: ${total_profit:.2f}\")\n",
    "        print(f\"  ROI: {roi:.2f}%\")\n",
    "        \n",
    "simulate_edge_betting(orig_test_preds)\n",
    "\n",
    "# More realistic simulation - random half of bets lose\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"More Realistic Simulation (random outcomes):\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "np.random.seed(42)\n",
    "bankroll = 1000.0\n",
    "bets_count = 0\n",
    "wins = 0\n",
    "\n",
    "for i, p in enumerate(orig_test_preds):\n",
    "    if p > 0.55:  # Only bet when model is confident\n",
    "        # Simulated outcome based on our probability\n",
    "        won = np.random.random() < p\n",
    "        odds = 1.9  # Typical odds\n",
    "        stake = bankroll * 0.02  # Fixed 2% of bankroll\n",
    "        \n",
    "        if won:\n",
    "            profit = stake * (odds - 1)\n",
    "            wins += 1\n",
    "        else:\n",
    "            profit = -stake\n",
    "        \n",
    "        bankroll += profit\n",
    "        bets_count += 1\n",
    "\n",
    "print(f\"  Bets placed: {bets_count}\")\n",
    "print(f\"  Win rate: {wins/bets_count*100:.1f}%\")\n",
    "print(f\"  Final bankroll: ${bankroll:.2f}\")\n",
    "print(f\"  ROI: {(bankroll - 1000) / (bets_count * 20) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f18e3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "FLAT STAKE BETTING SIMULATION\n",
      "==================================================\n",
      "\n",
      "Results with flat $100 stakes:\n",
      "  Bets placed: 8494\n",
      "  Wins: 5606 (66.0%)\n",
      "  Total staked: $849,400\n",
      "  Total profit: $215,740.00\n",
      "  ROI: 25.40%\n"
     ]
    }
   ],
   "source": [
    "# Flat stake betting simulation - realistic ROI calculation\n",
    "print(\"=\" * 50)\n",
    "print(\"FLAT STAKE BETTING SIMULATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "np.random.seed(42)\n",
    "stake_per_bet = 100  # $100 per bet\n",
    "total_profit = 0\n",
    "bets_placed = 0\n",
    "wins = 0\n",
    "\n",
    "# Use augmented test set where we know actual outcomes\n",
    "# In augmented data: winner=1 means player 1 won, winner=0 means player 1 lost\n",
    "\n",
    "results = []\n",
    "for i in range(len(test_preds)):\n",
    "    pred = test_preds[i]\n",
    "    actual = y_test[i]\n",
    "    \n",
    "    # Only bet when we have confidence > 55%\n",
    "    if pred > 0.55:\n",
    "        # Bet on player 1\n",
    "        odds = 1.9\n",
    "        if actual == 1:\n",
    "            profit = stake_per_bet * (odds - 1)\n",
    "            wins += 1\n",
    "        else:\n",
    "            profit = -stake_per_bet\n",
    "        total_profit += profit\n",
    "        bets_placed += 1\n",
    "        results.append({'pred': pred, 'actual': actual, 'profit': profit})\n",
    "    elif pred < 0.45:\n",
    "        # Bet on player 2 (opposite)\n",
    "        odds = 1.9\n",
    "        if actual == 0:\n",
    "            profit = stake_per_bet * (odds - 1)\n",
    "            wins += 1\n",
    "        else:\n",
    "            profit = -stake_per_bet\n",
    "        total_profit += profit\n",
    "        bets_placed += 1\n",
    "        results.append({'pred': pred, 'actual': actual, 'profit': profit})\n",
    "\n",
    "total_staked = bets_placed * stake_per_bet\n",
    "roi = total_profit / total_staked * 100 if total_staked > 0 else 0\n",
    "\n",
    "print(f\"\\nResults with flat $100 stakes:\")\n",
    "print(f\"  Bets placed: {bets_placed}\")\n",
    "print(f\"  Wins: {wins} ({wins/bets_placed*100:.1f}%)\")\n",
    "print(f\"  Total staked: ${total_staked:,.0f}\")\n",
    "print(f\"  Total profit: ${total_profit:,.2f}\")\n",
    "print(f\"  ROI: {roi:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "386d0e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Neural network ensemble saved to ml_models/neural_network_ensemble.pkl\n",
      "   - 20 models\n",
      "   - 17 features\n",
      "   - Test accuracy: 0.6387\n",
      "   - Test log loss: 0.6391\n"
     ]
    }
   ],
   "source": [
    "# Save the trained ensemble\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "os.makedirs('ml_models', exist_ok=True)\n",
    "\n",
    "# Save models and scaler\n",
    "nn_data = {\n",
    "    'models': [model.state_dict() for model in ensemble_models],\n",
    "    'scaler': scaler,\n",
    "    'features': available_features,\n",
    "    'hidden_dim': 100,\n",
    "    'n_models': len(ensemble_models),\n",
    "    'train_metrics': {\n",
    "        'accuracy': orig_accuracy,\n",
    "        'log_loss': orig_log_loss,\n",
    "        'roi': 25.40  # From simulation\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('ml_models/neural_network_ensemble.pkl', 'wb') as f:\n",
    "    pickle.dump(nn_data, f)\n",
    "\n",
    "print(\"✅ Neural network ensemble saved to ml_models/neural_network_ensemble.pkl\")\n",
    "print(f\"   - {len(ensemble_models)} models\")\n",
    "print(f\"   - {len(available_features)} features\")\n",
    "print(f\"   - Test accuracy: {orig_accuracy:.4f}\")\n",
    "print(f\"   - Test log loss: {orig_log_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fb8030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "df_features['year'] = df_features['tournament_date'].str[:4]\n",
    "\n",
    "train_df = df_features[df_features['year'].isin(['2020', '2021'])].copy()\n",
    "val_df = df_features[df_features['year'] == '2022'].copy()\n",
    "test_df = df_features[df_features['year'].isin(['2023', '2024'])].copy()\n",
    "\n",
    "print(\"Data split:\")\n",
    "print(f\"  Training (2020-2021):   {len(train_df):,}\")\n",
    "print(f\"  Validation (2022):      {len(val_df):,}\")\n",
    "print(f\"  Test (2023-2024):       {len(test_df):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525f0804",
   "metadata": {},
   "source": [
    "## 2. Prepare Features (Exclude RANK, POINTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3bfa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names\n",
    "all_cols = df_features.columns.tolist()\n",
    "feature_cols = [col for col in all_cols if col.startswith('player1_')]\n",
    "feature_names = [col.replace('player1_', '') for col in feature_cols]\n",
    "\n",
    "# Exclude RANK and POINTS\n",
    "excluded_features = ['RANK', 'POINTS']\n",
    "available_features = [f for f in feature_names if f not in excluded_features]\n",
    "\n",
    "print(f\"Total features: {len(feature_names)}\")\n",
    "print(f\"Excluded: {excluded_features}\")\n",
    "print(f\"Available: {len(available_features)}\")\n",
    "print(f\"\\nFeatures: {available_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f8c61a",
   "metadata": {},
   "source": [
    "## 3. Train Single Neural Network (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bfc607",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training single neural network...\\n\")\n",
    "\n",
    "single_nn = NeuralNetworkTrainer(\n",
    "    n_features=len(available_features),\n",
    "    learning_rate=0.0004,\n",
    "    momentum=0.55,\n",
    "    weight_decay=0.002,\n",
    "    patience=10,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "single_nn.fit(train_df, val_df, available_features, max_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d78647d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate single model\n",
    "from sklearn.metrics import accuracy_score, log_loss, brier_score_loss\n",
    "\n",
    "test_pred_single = single_nn.predict(test_df, available_features)\n",
    "test_actuals = (test_df['winner'] == 1).astype(int).values\n",
    "\n",
    "single_accuracy = accuracy_score(test_actuals, test_pred_single.round())\n",
    "single_log_loss = log_loss(test_actuals, test_pred_single)\n",
    "single_brier = brier_score_loss(test_actuals, test_pred_single)\n",
    "\n",
    "print(\"\\nSingle Neural Network Performance:\")\n",
    "print(f\"  Accuracy:    {single_accuracy:.2%}\")\n",
    "print(f\"  Log Loss:    {single_log_loss:.4f}\")\n",
    "print(f\"  Brier Score: {single_brier:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79921626",
   "metadata": {},
   "source": [
    "## 4. Train Ensemble with Bagging (20 Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3292fa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ensemble of 20 neural networks\n",
    "ensemble_models, ensemble_stats = train_nn_ensemble(\n",
    "    train_df,\n",
    "    val_df,\n",
    "    available_features,\n",
    "    n_bags=20,\n",
    "    learning_rate=0.0004,\n",
    "    momentum=0.55,\n",
    "    weight_decay=0.002,\n",
    "    patience=10,\n",
    "    max_epochs=100,\n",
    "    verbose=False  # Set to False to reduce output\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15402853",
   "metadata": {},
   "source": [
    "## 5. Evaluate Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021a1e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble predictions\n",
    "test_pred_ensemble = predict_ensemble(ensemble_models, test_df, available_features)\n",
    "\n",
    "ensemble_accuracy = accuracy_score(test_actuals, test_pred_ensemble.round())\n",
    "ensemble_log_loss = log_loss(test_actuals, test_pred_ensemble)\n",
    "ensemble_brier = brier_score_loss(test_actuals, test_pred_ensemble)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ENSEMBLE VS SINGLE MODEL COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n{'Metric':<20} {'Single NN':<15} {'Ensemble (20)':<15} {'Improvement'}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Accuracy':<20} {single_accuracy:<15.2%} {ensemble_accuracy:<15.2%} {(ensemble_accuracy-single_accuracy)*100:+.2f}%\")\n",
    "print(f\"{'Log Loss':<20} {single_log_loss:<15.4f} {ensemble_log_loss:<15.4f} {(single_log_loss-ensemble_log_loss):+.4f}\")\n",
    "print(f\"{'Brier Score':<20} {single_brier:<15.4f} {ensemble_brier:<15.4f} {(single_brier-ensemble_brier):+.4f}\")\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6370747",
   "metadata": {},
   "source": [
    "## 6. Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8693321b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Single model learning curve\n",
    "single_history = single_nn.get_history()\n",
    "epochs_single = range(1, len(single_history['train_loss']) + 1)\n",
    "\n",
    "ax1.plot(epochs_single, single_history['train_loss'], 'b-', label='Training Loss', linewidth=2)\n",
    "ax1.plot(epochs_single, single_history['val_loss'], 'r-', label='Validation Loss', linewidth=2)\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Loss (Binary Cross-Entropy)', fontsize=12)\n",
    "ax1.set_title('Single Neural Network - Learning Curve', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Ensemble learning curves (all models)\n",
    "for i, (train_losses, val_losses) in enumerate(zip(ensemble_stats['train_losses'], \n",
    "                                                     ensemble_stats['val_losses'])):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    ax2.plot(epochs, train_losses, 'b-', alpha=0.2, linewidth=1)\n",
    "    ax2.plot(epochs, val_losses, 'r-', alpha=0.2, linewidth=1)\n",
    "\n",
    "# Average curves\n",
    "max_epochs = max([len(losses) for losses in ensemble_stats['train_losses']])\n",
    "avg_train = []\n",
    "avg_val = []\n",
    "for epoch in range(max_epochs):\n",
    "    train_at_epoch = [losses[epoch] for losses in ensemble_stats['train_losses'] \n",
    "                     if epoch < len(losses)]\n",
    "    val_at_epoch = [losses[epoch] for losses in ensemble_stats['val_losses'] \n",
    "                   if epoch < len(losses)]\n",
    "    avg_train.append(np.mean(train_at_epoch))\n",
    "    avg_val.append(np.mean(val_at_epoch))\n",
    "\n",
    "epochs_avg = range(1, len(avg_train) + 1)\n",
    "ax2.plot(epochs_avg, avg_train, 'b-', label='Avg Training Loss', linewidth=3)\n",
    "ax2.plot(epochs_avg, avg_val, 'r-', label='Avg Validation Loss', linewidth=3)\n",
    "\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Loss (Binary Cross-Entropy)', fontsize=12)\n",
    "ax2.set_title('Ensemble (20 Models) - Learning Curves', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('nn_learning_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Learning curves saved: nn_learning_curves.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4c7ccf",
   "metadata": {},
   "source": [
    "## 7. Feature Importance (Permutation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32cda1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate feature importance\n",
    "importance_df = calculate_permutation_importance(\n",
    "    ensemble_models,\n",
    "    val_df,\n",
    "    available_features,\n",
    "    n_repeats=5\n",
    ")\n",
    "\n",
    "print(\"\\nFeature Importance (Permutation):\")\n",
    "print(importance_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f0eaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "colors = plt.cm.RdYlGn(np.linspace(0.2, 0.8, len(importance_df)))\n",
    "bars = ax.barh(importance_df['feature'], importance_df['importance'], \n",
    "              color=colors, alpha=0.7, edgecolor='black')\n",
    "\n",
    "# Add error bars\n",
    "ax.errorbar(importance_df['importance'], importance_df['feature'],\n",
    "           xerr=importance_df['std'], fmt='none', ecolor='black', \n",
    "           capsize=3, alpha=0.5)\n",
    "\n",
    "ax.set_xlabel('Importance (Increase in Log-Loss)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Neural Network Feature Importance\\n(via Permutation)', \n",
    "            fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('nn_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Feature importance plot saved: nn_feature_importance.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febd036e",
   "metadata": {},
   "source": [
    "## 8. Prediction Distribution & Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3e5bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration analysis\n",
    "bins = np.linspace(0, 1, 11)\n",
    "bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "\n",
    "bin_indices = np.digitize(test_pred_ensemble, bins) - 1\n",
    "bin_indices = np.clip(bin_indices, 0, len(bin_centers) - 1)\n",
    "\n",
    "calibration_data = []\n",
    "for i in range(len(bin_centers)):\n",
    "    mask = bin_indices == i\n",
    "    if mask.sum() > 0:\n",
    "        actual_rate = test_actuals[mask].mean()\n",
    "        predicted_rate = test_pred_ensemble[mask].mean()\n",
    "        count = mask.sum()\n",
    "        calibration_data.append({\n",
    "            'predicted': predicted_rate,\n",
    "            'actual': actual_rate,\n",
    "            'count': count\n",
    "        })\n",
    "\n",
    "calib_df = pd.DataFrame(calibration_data)\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Calibration curve\n",
    "ax1.plot([0, 1], [0, 1], 'r--', linewidth=2, label='Perfect Calibration')\n",
    "ax1.scatter(calib_df['predicted'], calib_df['actual'], \n",
    "           s=calib_df['count']*2, alpha=0.6, color='blue')\n",
    "ax1.plot(calib_df['predicted'], calib_df['actual'], 'b-', linewidth=1, alpha=0.5)\n",
    "\n",
    "ax1.set_xlabel('Predicted Probability', fontsize=12)\n",
    "ax1.set_ylabel('Actual Win Rate', fontsize=12)\n",
    "ax1.set_title('Calibration Curve\\n(size = number of matches)', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xlim(0, 1)\n",
    "ax1.set_ylim(0, 1)\n",
    "\n",
    "# Prediction distribution\n",
    "ax2.hist(test_pred_ensemble, bins=30, alpha=0.7, color='blue', edgecolor='black')\n",
    "ax2.axvline(x=0.5, color='red', linestyle='--', linewidth=2, label='50% threshold')\n",
    "ax2.set_xlabel('Predicted Probability', fontsize=12)\n",
    "ax2.set_ylabel('Frequency', fontsize=12)\n",
    "ax2.set_title('Ensemble Prediction Distribution', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('nn_calibration.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Calibration plot saved: nn_calibration.png\")\n",
    "\n",
    "calib_error = np.abs(calib_df['predicted'] - calib_df['actual']).mean()\n",
    "print(f\"\\nMean Calibration Error: {calib_error:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77e92a0",
   "metadata": {},
   "source": [
    "## 9. Performance by Surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca014e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get surface information\n",
    "test_with_surface = test_df.merge(matches[['match_id', 'surface']], on='match_id')\n",
    "\n",
    "surface_results = []\n",
    "\n",
    "for surface in ['Hard', 'Clay', 'Grass']:\n",
    "    mask = test_with_surface['surface'] == surface\n",
    "    if mask.sum() == 0:\n",
    "        continue\n",
    "    \n",
    "    surface_probs = test_pred_ensemble[mask]\n",
    "    surface_actuals = test_actuals[mask]\n",
    "    \n",
    "    surface_results.append({\n",
    "        'Surface': surface,\n",
    "        'Matches': mask.sum(),\n",
    "        'Accuracy': accuracy_score(surface_actuals, surface_probs.round()),\n",
    "        'Log Loss': log_loss(surface_actuals, surface_probs)\n",
    "    })\n",
    "\n",
    "surface_df = pd.DataFrame(surface_results)\n",
    "\n",
    "print(\"\\nPerformance by Surface:\")\n",
    "print(surface_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d936f719",
   "metadata": {},
   "source": [
    "## 10. Save Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63db9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save ensemble\n",
    "ensemble_data = {\n",
    "    'models': ensemble_models,\n",
    "    'features': available_features,\n",
    "    'ensemble_stats': ensemble_stats,\n",
    "    'test_metrics': {\n",
    "        'accuracy': ensemble_accuracy,\n",
    "        'log_loss': ensemble_log_loss,\n",
    "        'brier_score': ensemble_brier,\n",
    "        'calibration_error': calib_error\n",
    "    },\n",
    "    'feature_importance': importance_df\n",
    "}\n",
    "\n",
    "with open('ml_models/nn_ensemble.pkl', 'wb') as f:\n",
    "    pickle.dump(ensemble_data, f)\n",
    "\n",
    "print(\"✅ Ensemble saved: ml_models/nn_ensemble.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a97e52",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74d3740a",
   "metadata": {},
   "source": [
    "## 11. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92667e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"NEURAL NETWORK ENSEMBLE - FINAL REPORT\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n📅 Training completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"\\n🏗️  Architecture:\")\n",
    "print(f\"  Input features: {len(available_features)}\")\n",
    "print(f\"  Hidden neurons: 100 (tanh activation)\")\n",
    "print(f\"  Output: 1 (sigmoid activation)\")\n",
    "print(f\"  Bias terms: None (symmetric design)\")\n",
    "print(f\"\\n🎯 Training Configuration:\")\n",
    "print(f\"  Optimizer: SGD (momentum=0.55)\")\n",
    "print(f\"  Learning rate: 0.0004\")\n",
    "print(f\"  Weight decay: 0.002\")\n",
    "print(f\"  Batch size: 1 (online learning)\")\n",
    "print(f\"  Early stopping: patience=10\")\n",
    "print(f\"\\n🎲 Ensemble:\")\n",
    "print(f\"  Number of models: {len(ensemble_models)}\")\n",
    "print(f\"  Bagging: Bootstrap sampling\")\n",
    "print(f\"  Prediction: Average of all models\")\n",
    "print(f\"\\n📊 Test Set Performance (2023-2024):\")\n",
    "print(f\"  Test samples: {len(test_df):,}\")\n",
    "print(f\"  Accuracy:      {ensemble_accuracy:.2%}\")\n",
    "print(f\"  Log Loss:      {ensemble_log_loss:.4f}\")\n",
    "print(f\"  Brier Score:   {ensemble_brier:.4f}\")\n",
    "print(f\"  Calibration:   {calib_error:.4f}\")\n",
    "print(f\"\\n📈 Improvement over Single Model:\")\n",
    "print(f\"  Accuracy:      {(ensemble_accuracy-single_accuracy)*100:+.2f}%\")\n",
    "print(f\"  Log Loss:      {(single_log_loss-ensemble_log_loss):+.4f}\")\n",
    "print(f\"  Brier Score:   {(single_brier-ensemble_brier):+.4f}\")\n",
    "print(f\"\\n📁 Files Generated:\")\n",
    "print(f\"  ✅ nn_learning_curves.png\")\n",
    "print(f\"  ✅ nn_feature_importance.png\")\n",
    "print(f\"  ✅ nn_calibration.png\")\n",
    "print(f\"  ✅ ml_models/nn_ensemble.pkl\")\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44ede0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close connections\n",
    "conn.close()\n",
    "feature_gen.close()\n",
    "print(\"\\n✅ Database connections closed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
