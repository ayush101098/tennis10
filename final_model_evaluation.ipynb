{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59197ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "# Import custom modules\n",
    "from features import MatchFeatures\n",
    "from hierarchical_model import HierarchicalTennisModel\n",
    "from evaluation.model_comparison import (\n",
    "    calculate_log_loss,\n",
    "    calculate_brier_score,\n",
    "    calculate_accuracy,\n",
    "    calculate_calibration_curve,\n",
    "    plot_calibration_curve,\n",
    "    plot_reliability_diagram,\n",
    "    evaluate_model,\n",
    "    compare_all_models,\n",
    "    statistical_significance_tests,\n",
    "    plot_model_comparison\n",
    ")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "print(\"‚úÖ Libraries loaded\")\n",
    "print(f\"Evaluation started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a7e5f5",
   "metadata": {},
   "source": [
    "## 1. Load Test Data (2023-2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789c39cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to database\n",
    "conn = sqlite3.connect('tennis_data.db')\n",
    "\n",
    "# Load test matches\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    m.match_id,\n",
    "    m.tournament_date,\n",
    "    m.surface,\n",
    "    m.winner_id,\n",
    "    m.loser_id,\n",
    "    m.best_of,\n",
    "    CASE WHEN m.winner_id < m.loser_id THEN 1 ELSE 2 END as actual_winner\n",
    "FROM matches m\n",
    "WHERE m.tournament_date >= '2023-01-01'\n",
    "    AND m.tournament_date < '2025-01-01'\n",
    "    AND m.surface IS NOT NULL\n",
    "ORDER BY m.tournament_date\n",
    "\"\"\"\n",
    "\n",
    "test_matches = pd.read_sql_query(query, conn)\n",
    "\n",
    "print(f\"Test matches: {len(test_matches):,}\")\n",
    "print(f\"Date range: {test_matches['tournament_date'].min()} to {test_matches['tournament_date'].max()}\")\n",
    "print(f\"\\nSurface distribution:\")\n",
    "print(test_matches['surface'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee5270d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate features for test matches\n",
    "feature_gen = MatchFeatures('tennis_data.db')\n",
    "\n",
    "print(\"Generating features for test matches...\\n\")\n",
    "\n",
    "features_list = []\n",
    "\n",
    "for idx, match in test_matches.iterrows():\n",
    "    if idx % 500 == 0:\n",
    "        print(f\"Processing match {idx}/{len(test_matches)}...\")\n",
    "    \n",
    "    # Ensure player1_id < player2_id\n",
    "    if match['winner_id'] < match['loser_id']:\n",
    "        player1_id = match['winner_id']\n",
    "        player2_id = match['loser_id']\n",
    "    else:\n",
    "        player1_id = match['loser_id']\n",
    "        player2_id = match['winner_id']\n",
    "    \n",
    "    features = feature_gen.generate_features(\n",
    "        player1_id,\n",
    "        player2_id,\n",
    "        match['surface'],\n",
    "        match_date=match['tournament_date']\n",
    "    )\n",
    "    \n",
    "    features['match_id'] = match['match_id']\n",
    "    features['tournament_date'] = match['tournament_date']\n",
    "    features['actual_winner'] = match['actual_winner']\n",
    "    \n",
    "    features_list.append(features)\n",
    "\n",
    "test_df = pd.DataFrame(features_list)\n",
    "\n",
    "print(f\"\\n‚úÖ Generated features for {len(test_df)} matches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0bad55",
   "metadata": {},
   "source": [
    "## 2. Generate Simulated Odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e304556d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_realistic_odds(test_df: pd.DataFrame, margin: float = 0.05) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate realistic betting odds with bookmaker margin.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    odds_data = []\n",
    "    \n",
    "    for _, row in test_df.iterrows():\n",
    "        # Use ranking difference to estimate true probability\n",
    "        rank_diff = row.get('player1_RANK', 50) - row.get('player2_RANK', 50)\n",
    "        \n",
    "        # Convert to probability (sigmoid)\n",
    "        p_true = 1 / (1 + np.exp(rank_diff / 30))\n",
    "        \n",
    "        # Add noise\n",
    "        p_true = np.clip(p_true + np.random.normal(0, 0.05), 0.1, 0.9)\n",
    "        \n",
    "        # Apply bookmaker margin\n",
    "        p1_implied = p_true * (1 + margin)\n",
    "        p2_implied = (1 - p_true) * (1 + margin)\n",
    "        \n",
    "        # Convert to decimal odds\n",
    "        player1_odds = 1 / p1_implied\n",
    "        player2_odds = 1 / p2_implied\n",
    "        \n",
    "        odds_data.append({\n",
    "            'match_id': row['match_id'],\n",
    "            'player1_odds': player1_odds,\n",
    "            'player2_odds': player2_odds\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(odds_data)\n",
    "\n",
    "odds_df = generate_realistic_odds(test_df)\n",
    "\n",
    "print(\"Simulated Odds Statistics:\")\n",
    "print(f\"  Player 1 Avg Odds: {odds_df['player1_odds'].mean():.2f}\")\n",
    "print(f\"  Player 2 Avg Odds: {odds_df['player2_odds'].mean():.2f}\")\n",
    "print(f\"  Min Odds: {min(odds_df['player1_odds'].min(), odds_df['player2_odds'].min()):.2f}\")\n",
    "print(f\"  Max Odds: {max(odds_df['player1_odds'].max(), odds_df['player2_odds'].max()):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d3e3a3",
   "metadata": {},
   "source": [
    "## 3. Load Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485fcf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Logistic Regression\n",
    "try:\n",
    "    with open('ml_models/logistic_model.pkl', 'rb') as f:\n",
    "        lr_data = pickle.load(f)\n",
    "    lr_model = lr_data['model']\n",
    "    lr_features = lr_data['selected_features']\n",
    "    print(f\"‚úÖ Logistic Regression loaded ({len(lr_features)} features)\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ö†Ô∏è  Logistic Regression not found - run logistic_regression_model.ipynb first\")\n",
    "    lr_model = None\n",
    "\n",
    "# Load Neural Network Ensemble\n",
    "try:\n",
    "    with open('ml_models/nn_ensemble.pkl', 'rb') as f:\n",
    "        nn_data = pickle.load(f)\n",
    "    nn_models = nn_data['models']\n",
    "    nn_features = nn_data['features']\n",
    "    print(f\"‚úÖ Neural Network Ensemble loaded ({len(nn_models)} models, {len(nn_features)} features)\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ö†Ô∏è  Neural Network not found - run neural_network_model.ipynb first\")\n",
    "    nn_models = None\n",
    "\n",
    "# Initialize Markov Model\n",
    "markov_model = HierarchicalTennisModel('tennis_data.db')\n",
    "print(\"‚úÖ Markov Model initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e7aeb3",
   "metadata": {},
   "source": [
    "## 4. Generate Predictions from All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ae48f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions from all models\n",
    "predictions = {\n",
    "    'match_id': test_df['match_id'],\n",
    "    'actual_winner': test_df['actual_winner']\n",
    "}\n",
    "\n",
    "# 1. Markov Model predictions\n",
    "print(\"Generating Markov Model predictions...\")\n",
    "markov_probs = []\n",
    "for _, match in test_matches.iterrows():\n",
    "    result = markov_model.predict_match(\n",
    "        match['winner_id'],\n",
    "        match['loser_id'],\n",
    "        match['surface'],\n",
    "        match['best_of'],\n",
    "        match_date=match['tournament_date']\n",
    "    )\n",
    "    # Adjust for player ordering (ensure player1_id < player2_id)\n",
    "    if match['winner_id'] < match['loser_id']:\n",
    "        p = result['p_player1_win']\n",
    "    else:\n",
    "        p = result['p_player2_win']\n",
    "    markov_probs.append(p)\n",
    "\n",
    "predictions['Markov'] = markov_probs\n",
    "print(f\"  ‚úÖ Mean prediction: {np.mean(markov_probs):.3f}\")\n",
    "print(f\"  ‚úÖ Accuracy: {calculate_accuracy(np.array(markov_probs), test_df['actual_winner'].values):.2%}\")\n",
    "\n",
    "# 2. Logistic Regression predictions\n",
    "if lr_model is not None:\n",
    "    print(\"\\nGenerating Logistic Regression predictions...\")\n",
    "    test_df_copy = test_df.copy()\n",
    "    test_df_copy['winner'] = test_df_copy['actual_winner']\n",
    "    lr_probs = lr_model.predict_proba(test_df_copy)\n",
    "    predictions['Logistic'] = lr_probs\n",
    "    print(f\"  ‚úÖ Mean prediction: {np.mean(lr_probs):.3f}\")\n",
    "    print(f\"  ‚úÖ Accuracy: {calculate_accuracy(lr_probs, test_df['actual_winner'].values):.2%}\")\n",
    "else:\n",
    "    predictions['Logistic'] = [0.5] * len(test_df)\n",
    "    print(\"‚ö†Ô∏è  Using baseline predictions for Logistic Regression\")\n",
    "\n",
    "# 3. Neural Network Ensemble predictions\n",
    "if nn_models is not None:\n",
    "    print(\"\\nGenerating Neural Network Ensemble predictions...\")\n",
    "    from ml_models.neural_network import predict_ensemble\n",
    "    nn_probs = predict_ensemble(nn_models, test_df, nn_features)\n",
    "    predictions['Neural Net'] = nn_probs\n",
    "    print(f\"  ‚úÖ Mean prediction: {np.mean(nn_probs):.3f}\")\n",
    "    print(f\"  ‚úÖ Accuracy: {calculate_accuracy(nn_probs, test_df['actual_winner'].values):.2%}\")\n",
    "else:\n",
    "    predictions['Neural Net'] = [0.5] * len(test_df)\n",
    "    print(\"‚ö†Ô∏è  Using baseline predictions for Neural Network\")\n",
    "\n",
    "# 4. Hybrid Ensemble (weighted average)\n",
    "if lr_model is not None and nn_models is not None:\n",
    "    print(\"\\nCreating Hybrid Ensemble...\")\n",
    "    # Weights optimized on validation set\n",
    "    weights = {\n",
    "        'Markov': 0.20,\n",
    "        'Logistic': 0.35,\n",
    "        'Neural Net': 0.45\n",
    "    }\n",
    "    \n",
    "    ensemble_probs = (\n",
    "        weights['Markov'] * np.array(predictions['Markov']) +\n",
    "        weights['Logistic'] * np.array(predictions['Logistic']) +\n",
    "        weights['Neural Net'] * np.array(predictions['Neural Net'])\n",
    "    )\n",
    "    predictions['Hybrid'] = ensemble_probs\n",
    "    print(f\"  ‚úÖ Weights: Markov={weights['Markov']}, Logistic={weights['Logistic']}, NN={weights['Neural Net']}\")\n",
    "    print(f\"  ‚úÖ Mean prediction: {np.mean(ensemble_probs):.3f}\")\n",
    "    print(f\"  ‚úÖ Accuracy: {calculate_accuracy(ensemble_probs, test_df['actual_winner'].values):.2%}\")\n",
    "\n",
    "predictions_df = pd.DataFrame(predictions)\n",
    "print(f\"\\n‚úÖ Generated predictions for {len(predictions_df)} matches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c1efd0",
   "metadata": {},
   "source": [
    "## 5. Evaluate All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44d658e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare predictions dictionary for evaluation\n",
    "predictions_dict = {\n",
    "    model: np.array(predictions_df[model])\n",
    "    for model in predictions_df.columns\n",
    "    if model not in ['match_id', 'actual_winner']\n",
    "}\n",
    "\n",
    "actuals = predictions_df['actual_winner'].values\n",
    "\n",
    "# Comprehensive evaluation\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"EVALUATING ALL MODELS ON TEST SET (2023-2024)\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "comparison_df, results = compare_all_models(\n",
    "    predictions_dict,\n",
    "    actuals,\n",
    "    odds_df,\n",
    "    initial_bankroll=1000.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406e65de",
   "metadata": {},
   "source": [
    "## 6. Results Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c077a7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"MODEL COMPARISON RESULTS\")\n",
    "print(\"=\"*100)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Identify best model\n",
    "best_model_idx = comparison_df['ROI (Kelly)'].apply(lambda x: float(x.strip('%+'))).idxmax()\n",
    "best_model = comparison_df.iloc[best_model_idx]['Model']\n",
    "\n",
    "print(f\"\\nüèÜ BEST MODEL: {best_model}\")\n",
    "print(f\"   ROI: {comparison_df.iloc[best_model_idx]['ROI (Kelly)']}\")\n",
    "print(f\"   Sharpe: {comparison_df.iloc[best_model_idx]['Sharpe']}\")\n",
    "print(f\"   Max DD: {comparison_df.iloc[best_model_idx]['Max DD']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3988f2f",
   "metadata": {},
   "source": [
    "## 7. Statistical Significance Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543af6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairwise statistical tests\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"STATISTICAL SIGNIFICANCE TESTS (McNemar's Test)\")\n",
    "print(\"=\"*100)\n",
    "print(\"Significance levels: *** p<0.01, ** p<0.05, * p<0.10, n.s. not significant\")\n",
    "print(\"=\"*100 + \"\\n\")\n",
    "\n",
    "significance_df = statistical_significance_tests(predictions_dict, actuals)\n",
    "print(significance_df.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd23473",
   "metadata": {},
   "source": [
    "## 8. Model Comparison Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e2ce87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive comparison plot\n",
    "plot_model_comparison(\n",
    "    results,\n",
    "    save_path='evaluation_model_comparison.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e02b88",
   "metadata": {},
   "source": [
    "## 9. Calibration Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd64948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot calibration curves for all models\n",
    "plot_calibration_curve(\n",
    "    predictions_dict,\n",
    "    actuals,\n",
    "    n_bins=10,\n",
    "    save_path='evaluation_calibration_curves.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fef2e2",
   "metadata": {},
   "source": [
    "## 10. Individual Reliability Diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790d168b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot reliability diagram for each model\n",
    "for model_name, preds in predictions_dict.items():\n",
    "    plot_reliability_diagram(\n",
    "        preds,\n",
    "        actuals,\n",
    "        model_name=model_name,\n",
    "        n_bins=10,\n",
    "        save_path=f'evaluation_reliability_{model_name.lower().replace(\" \", \"_\")}.png'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7eafca",
   "metadata": {},
   "source": [
    "## 11. Probability Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c9a044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of predictions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (model_name, preds) in enumerate(predictions_dict.items()):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Histogram\n",
    "    ax.hist(preds, bins=50, alpha=0.7, edgecolor='black', color='steelblue')\n",
    "    \n",
    "    # Statistics\n",
    "    mean_pred = np.mean(preds)\n",
    "    median_pred = np.median(preds)\n",
    "    std_pred = np.std(preds)\n",
    "    \n",
    "    ax.axvline(mean_pred, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_pred:.3f}')\n",
    "    ax.axvline(median_pred, color='green', linestyle='--', linewidth=2, label=f'Median: {median_pred:.3f}')\n",
    "    \n",
    "    ax.set_xlabel('Predicted Probability', fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
    "    ax.set_title(f'{model_name} - Prediction Distribution (œÉ={std_pred:.3f})', fontsize=12, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('evaluation_prediction_distributions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Prediction distribution plot saved: evaluation_prediction_distributions.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c020347",
   "metadata": {},
   "source": [
    "## 12. Performance by Surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8fa076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze performance by surface\n",
    "test_with_surface = test_df.copy()\n",
    "test_with_surface['surface'] = test_matches['surface'].values\n",
    "\n",
    "surface_results = []\n",
    "\n",
    "for surface in ['Hard', 'Clay', 'Grass']:\n",
    "    mask = test_with_surface['surface'] == surface\n",
    "    \n",
    "    if mask.sum() == 0:\n",
    "        continue\n",
    "    \n",
    "    for model_name, preds in predictions_dict.items():\n",
    "        surface_preds = preds[mask]\n",
    "        surface_actuals = actuals[mask]\n",
    "        \n",
    "        accuracy = calculate_accuracy(surface_preds, surface_actuals)\n",
    "        logloss = calculate_log_loss(surface_preds, surface_actuals)\n",
    "        \n",
    "        surface_results.append({\n",
    "            'Surface': surface,\n",
    "            'Model': model_name,\n",
    "            'Matches': mask.sum(),\n",
    "            'Accuracy': f\"{accuracy:.2%}\",\n",
    "            'Log Loss': f\"{logloss:.4f}\"\n",
    "        })\n",
    "\n",
    "surface_df = pd.DataFrame(surface_results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PERFORMANCE BY SURFACE\")\n",
    "print(\"=\"*80)\n",
    "print(surface_df.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23061fc",
   "metadata": {},
   "source": [
    "## 13. Model Agreement Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ce07cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze when models agree/disagree\n",
    "model_predictions = {}\n",
    "for model_name, preds in predictions_dict.items():\n",
    "    model_predictions[model_name] = (preds > 0.5).astype(int) + 1  # Convert to winner (1 or 2)\n",
    "\n",
    "# Find matches where all models agree\n",
    "all_agree_mask = True\n",
    "model_names = list(model_predictions.keys())\n",
    "for i in range(len(model_names) - 1):\n",
    "    all_agree_mask &= (model_predictions[model_names[i]] == model_predictions[model_names[i+1]])\n",
    "\n",
    "n_agree = all_agree_mask.sum()\n",
    "n_total = len(actuals)\n",
    "\n",
    "# Accuracy when all agree\n",
    "agree_correct = (model_predictions[model_names[0]][all_agree_mask] == actuals[all_agree_mask]).sum()\n",
    "agree_accuracy = agree_correct / n_agree if n_agree > 0 else 0\n",
    "\n",
    "# Accuracy when models disagree\n",
    "disagree_mask = ~all_agree_mask\n",
    "if disagree_mask.sum() > 0:\n",
    "    disagree_correct = (model_predictions[model_names[0]][disagree_mask] == actuals[disagree_mask]).sum()\n",
    "    disagree_accuracy = disagree_correct / disagree_mask.sum()\n",
    "else:\n",
    "    disagree_accuracy = 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL AGREEMENT ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Matches where all models agree: {n_agree} / {n_total} ({n_agree/n_total:.1%})\")\n",
    "print(f\"  Accuracy when all agree: {agree_accuracy:.2%}\")\n",
    "print(f\"\\nMatches where models disagree: {disagree_mask.sum()} / {n_total} ({disagree_mask.sum()/n_total:.1%})\")\n",
    "print(f\"  Accuracy when disagree: {disagree_accuracy:.2%}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cdef2a",
   "metadata": {},
   "source": [
    "## 14. Final Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d61f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"FINAL EVALUATION SUMMARY REPORT\")\n",
    "print(\"=\"*100)\n",
    "print(f\"\\nüìÖ Report Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"\\nüìä Test Period: {test_matches['tournament_date'].min()} to {test_matches['tournament_date'].max()}\")\n",
    "print(f\"Test Matches: {len(test_matches):,}\")\n",
    "print(f\"Surfaces: {', '.join([f'{s} ({c})' for s, c in test_matches['surface'].value_counts().items()])}\")\n",
    "print(f\"\\nüí∞ Initial Bankroll: $1,000.00\")\n",
    "print(f\"Strategy: Kelly Criterion (25% fractional sizing, 5% max bet)\")\n",
    "print(f\"\\nüèÜ BEST PERFORMING MODEL: {best_model}\")\n",
    "print(f\"\\nüìà Key Metrics:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(f\"\\nüìä Statistical Significance:\")\n",
    "print(significance_df.to_string(index=False))\n",
    "print(f\"\\nüéØ Model Agreement:\")\n",
    "print(f\"   All models agree: {n_agree/n_total:.1%} of matches\")\n",
    "print(f\"   Accuracy when agree: {agree_accuracy:.2%}\")\n",
    "print(f\"   Accuracy when disagree: {disagree_accuracy:.2%}\")\n",
    "print(f\"\\nüìÅ Generated Files:\")\n",
    "print(f\"   ‚úÖ evaluation_model_comparison.png\")\n",
    "print(f\"   ‚úÖ evaluation_calibration_curves.png\")\n",
    "print(f\"   ‚úÖ evaluation_prediction_distributions.png\")\n",
    "for model_name in predictions_dict.keys():\n",
    "    print(f\"   ‚úÖ evaluation_reliability_{model_name.lower().replace(' ', '_')}.png\")\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "\n",
    "# Recommendation\n",
    "print(\"\\nüéØ RECOMMENDATION:\")\n",
    "print(f\"   Use {best_model} for production betting\")\n",
    "print(f\"   Expected ROI: {comparison_df.iloc[best_model_idx]['ROI (Kelly)']}\")\n",
    "print(f\"   Risk-adjusted performance (Sharpe): {comparison_df.iloc[best_model_idx]['Sharpe']}\")\n",
    "print(f\"   Maximum expected drawdown: {comparison_df.iloc[best_model_idx]['Max DD']}\")\n",
    "print(\"\\n\" + \"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a7a6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close connections\n",
    "conn.close()\n",
    "feature_gen.close()\n",
    "markov_model.close()\n",
    "print(\"\\n‚úÖ Database connections closed\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
